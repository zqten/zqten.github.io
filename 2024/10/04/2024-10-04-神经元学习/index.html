

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/inment.png">
  <link rel="icon" href="/img/inment.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="zqten">
  <meta name="keywords" content="">
  
    <meta name="description" content="第一步：了解神经网络的基本概念 :smile:矩阵点乘规则：当矩阵A的列数（column）等于矩阵B的行数（row）时，A与B可以相乘  2行3列 * 3行2列 &#x3D; 2行2列 （取第一个矩阵的行和第二个矩阵的列） 3行2列 * 2行3列 &#x3D; 3行3列 （取第一个矩阵的行和第二个矩阵的列）  神经网络是一种模拟生物神经网络的计算模型，用于处理复杂的模式识别问题。它们由一系列神经元（或称节">
<meta property="og:type" content="article">
<meta property="og:title" content="神经元学习">
<meta property="og:url" content="http://example.com/2024/10/04/2024-10-04-%E7%A5%9E%E7%BB%8F%E5%85%83%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="学无止境">
<meta property="og:description" content="第一步：了解神经网络的基本概念 :smile:矩阵点乘规则：当矩阵A的列数（column）等于矩阵B的行数（row）时，A与B可以相乘  2行3列 * 3行2列 &#x3D; 2行2列 （取第一个矩阵的行和第二个矩阵的列） 3行2列 * 2行3列 &#x3D; 3行3列 （取第一个矩阵的行和第二个矩阵的列）  神经网络是一种模拟生物神经网络的计算模型，用于处理复杂的模式识别问题。它们由一系列神经元（或称节">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-10-03T16:00:00.000Z">
<meta property="article:modified_time" content="2024-10-26T05:45:53.416Z">
<meta property="article:author" content="zqten">
<meta property="article:tag" content="编程">
<meta property="article:tag" content="python">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>神经元学习 - 学无止境</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-10-25T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>学无止境的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default1.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.1)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="神经元学习"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-10-04 00:00" pubdate>
          2024年10月4日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          14k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          114 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">神经元学习</h1>
            
            
              <div class="markdown-body">
                
                <h2
id="第一步了解神经网络的基本概念">第一步：了解神经网络的基本概念</h2>
<p>:smile:矩阵点乘规则：<strong>当矩阵A的列数（column）等于矩阵B的行数（row）时，A与B可以相乘</strong></p>
<ul>
<li>2行3列 * 3行2列 = 2行2列 （取第一个矩阵的行和第二个矩阵的列）</li>
<li>3行2列 * 2行3列 = 3行3列 （取第一个矩阵的行和第二个矩阵的列）</li>
</ul>
<p>神经网络是一种模拟生物神经网络的计算模型，用于处理复杂的模式识别问题。它们由一系列<strong>神经元</strong>（或称节点）组成，这些神经元通过<strong>权重</strong>相连，权重是神经网络中最重要的参数之一。每个神经元接收输入信号，并应用一个<strong>激活函数</strong>来决定输出信号。</p>
<h4 id="关键术语">关键术语：</h4>
<ol type="1">
<li><strong>神经元（Neuron）</strong>：基本的处理单元，接收输入并生成输出。</li>
<li><strong>权重（Weight）</strong>：连接神经元之间的参数，决定信号的强度。</li>
<li><strong>偏置（Bias）</strong>：一个神经元输出中独立于输入的项，帮助调整输出。</li>
<li><strong>激活函数（Activation
Function）</strong>：用于非线性转换神经元的输出，例如Sigmoid, Tanh,
ReLU。</li>
</ol>
<h4 id="一个简单的神经网络结构">一个简单的神经网络结构：</h4>
<figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sas">输入层（<span class="hljs-keyword">Input</span> Layer） → 隐藏层（Hidden Layer） → 输出层（<span class="hljs-keyword">Output</span> Layer）<br></code></pre></td></tr></table></figure>
<ul>
<li><strong>输入层</strong>：接收输入数据。</li>
<li><strong>隐藏层</strong>：中间层，可以有多层，用于提取特征。</li>
<li><strong>输出层</strong>：输出最终的结果。</li>
</ul>
<h3 id="练习">练习：</h3>
<ol type="1">
<li><p>请你简单描述神经网络是什么。</p></li>
<li><p>列出并解释神经网络的三个主要组件。</p>
<p>神经网络是一种模拟生物神经网络的计算模型，用于处理复杂的模式识别问题；三个主要组件是输入层、隐藏层、输出层</p></li>
</ol>
<h2
id="第二步理解一个简单神经元的工作原理">第二步：理解一个简单神经元的工作原理</h2>
<p>一个神经元接收输入、计算加权和并应用激活函数来生成输出。让我们更详细地看一下一个简单神经元的工作过程：</p>
<h4 id="神经元的数学表达">神经元的数学表达：</h4>
<p>假设我们有一个输入 ( x )，它经过权重 ( w ) 和偏置 ( b )
调整，然后通过激活函数 ( f ) 来生成输出 ( y )：</p>
<p>$ y = f(w x + b) $</p>
<p>其中： - ( x ) 是输入。 - ( w ) 是权重。 - ( b ) 是偏置。 - ( f )
是激活函数（例如：Sigmoid、ReLU 等）。</p>
<h4 id="常见激活函数">常见激活函数：</h4>
<ol type="1">
<li><p><strong>Sigmoid 函数</strong>：将输入映射到 (0) 和 (1) 之间。 $
(x) = $</p></li>
<li><p><strong>ReLU 函数</strong>：将负数部分映射为 (0)，正数部分不变。
$ (x) = (0, x) $</p></li>
</ol>
<h4 id="练习-1">练习：</h4>
<ol type="1">
<li>假设你有一个输入 ( x = 2 )，权重 ( w = 0.5 )，偏置 ( b = 1
)，并且使用 Sigmoid 激活函数。计算输出 ( y )。</li>
<li>为什么激活函数是必要的？</li>
</ol>
<p>输入 x=2，权重 w=0.5，偏置 b=1，计算输出 y。</p>
<p>计算步骤如下：</p>
<ul>
<li>首先，计算加权和及偏置： <span
class="math inline">\(w⋅x+b=0.5⋅2+1=1+1=2\)</span></li>
<li>然后，计算 Sigmoid 函数的值： <span class="math inline">\(\sigma(2)
= \frac{1}{1 + e^{-2}} \approx 0.88\)</span></li>
</ul>
<p>因此，输出$ y≈0.88$</p>
<h4 id="为什么激活函数是必要的">为什么激活函数是必要的？</h4>
<p>激活函数引入了非线性，使得神经网络能够学习和表示复杂的模式。如果没有激活函数，网络只会进行线性变换（输入的加权求和），这无法有效地处理复杂的模式或决策问题。</p>
<h2 id="第三步构建一个简单的神经元">第三步：构建一个简单的神经元</h2>
<p>接下来，我们将把刚才学到的知识应用于构建一个简单的神经元，并用 Python
代码来实现它。</p>
<h4 id="构建步骤">构建步骤：</h4>
<ol type="1">
<li><strong>定义神经元的结构</strong>：包括输入、权重、偏置和激活函数。</li>
<li><strong>实现加权和及偏置</strong>：计算 <span
class="math inline">\(( w \cdot x + b )\)</span>。</li>
<li><strong>应用激活函数</strong>：使用 Sigmoid 函数来计算输出。</li>
</ol>
<h3 id="python-代码实现">Python 代码实现</h3>
<p>以下是一个简单神经元的 Python 实现，包括输入、权重、偏置和 Sigmoid
激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 定义 Sigmoid 激活函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + math.exp(-x))<br><br><span class="hljs-comment"># 输入、权重和偏置</span><br>x = <span class="hljs-number">2</span>            <span class="hljs-comment"># 输入</span><br>w = <span class="hljs-number">0.5</span>          <span class="hljs-comment"># 权重</span><br>b = <span class="hljs-number">1</span>            <span class="hljs-comment"># 偏置</span><br><br><span class="hljs-comment"># 计算加权和及偏置</span><br>z = w * x + b<br><br><span class="hljs-comment"># 应用 Sigmoid 激活函数</span><br>y = sigmoid(z)<br><br><span class="hljs-comment"># 输出结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;输出 y = <span class="hljs-subst">&#123;y&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<h4 id="代码解释">代码解释：</h4>
<ol type="1">
<li>定义了一个 <code>sigmoid</code> 函数来计算 Sigmoid
激活函数的值。</li>
<li>设置了输入 ( x )、权重 ( w ) 和偏置 ( b )。</li>
<li>计算了加权和及偏置 ( z )。</li>
<li>使用 <code>sigmoid</code> 函数计算输出 ( y )。</li>
<li>打印输出结果。</li>
</ol>
<h3 id="练习-2">练习：</h3>
<ol type="1">
<li>请在你的 Python 环境中运行上述代码，观察输出。</li>
<li>尝试修改输入 ( x )、权重 ( w ) 和偏置 ( b )
的值，观察输出是如何变化的。</li>
</ol>
<h2
id="第四步构建一个简单的神经网络">第四步：构建一个简单的神经网络</h2>
<p>我们将构建一个包含输入层、一个隐藏层和输出层的简单神经网络。这个神经网络将接受两个输入，经过一个隐藏层处理后，生成一个输出。</p>
<h4 id="神经网络结构">神经网络结构：</h4>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">输入层 <span class="hljs-comment">(2 个输入)</span> → 隐藏层 <span class="hljs-comment">(2 个神经元)</span> → 输出层 <span class="hljs-comment">(1 个输出)</span><br></code></pre></td></tr></table></figure>
<h3 id="具体步骤">具体步骤：</h3>
<ol type="1">
<li><strong>初始化输入、权重和偏置</strong>。</li>
<li><strong>计算隐藏层的输出</strong>。</li>
<li><strong>计算输出层的输出</strong>。</li>
</ol>
<h3 id="python-代码实现-1">Python 代码实现</h3>
<p>以下代码展示了如何构建和运行这个简单的神经网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 定义 Sigmoid 激活函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-comment"># 输入</span><br>inputs = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])  <span class="hljs-comment"># 2 个输入</span><br><br><span class="hljs-comment"># 权重和偏置</span><br><span class="hljs-comment"># 隐藏层 (2 个神经元，每个神经元有 2 个输入)</span><br>hidden_weights = np.array([[<span class="hljs-number">0.5</span>, -<span class="hljs-number">0.6</span>], <br>                           [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>]])  <span class="hljs-comment"># 权重矩阵 2x2</span><br>hidden_bias = np.array([<span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>])      <span class="hljs-comment"># 偏置向量</span><br><br><span class="hljs-comment"># 输出层 (1 个神经元，2 个输入)</span><br>output_weights = np.array([<span class="hljs-number">0.4</span>, -<span class="hljs-number">0.7</span>])  <span class="hljs-comment"># 权重向量</span><br>output_bias = <span class="hljs-number">0.1</span>                       <span class="hljs-comment"># 偏置</span><br><br><span class="hljs-comment"># 计算隐藏层的输出</span><br>hidden_input = np.dot(hidden_weights, inputs) + hidden_bias<br>hidden_output = sigmoid(hidden_input)<br><br><span class="hljs-comment"># 计算输出层的输出</span><br>output_input = np.dot(output_weights, hidden_output) + output_bias<br>output = sigmoid(output_input)<br><br><span class="hljs-comment"># 输出结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;隐藏层输出: <span class="hljs-subst">&#123;hidden_output&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;最终输出: <span class="hljs-subst">&#123;output&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<h4 id="代码解释-1">代码解释：</h4>
<ol type="1">
<li><strong>定义 Sigmoid 激活函数</strong>：使用 <code>numpy</code> 的
<code>exp</code> 函数处理数组。</li>
<li><strong>初始化输入</strong>：<code>inputs</code> 是一个包含 2
个输入的数组。</li>
<li><strong>初始化隐藏层的权重和偏置</strong>：<code>hidden_weights</code>
是 2x2 的权重矩阵，<code>hidden_bias</code> 是包含 2 个偏置的数组。</li>
<li><strong>初始化输出层的权重和偏置</strong>：<code>output_weights</code>
是 1x2 的权重向量，<code>output_bias</code> 是一个单一的偏置值。</li>
<li><strong>计算隐藏层的输出</strong>：使用 <code>np.dot</code>
计算权重和输入的点积，加上偏置，再通过 <code>sigmoid</code>
函数处理。</li>
<li><strong>计算输出层的输出</strong>：类似地，使用隐藏层的输出计算输出层的最终输出。</li>
</ol>
<h3 id="练习-3">练习：</h3>
<ol type="1">
<li>请在你的 Python 环境中运行上述代码，并观察输出结果。</li>
<li>尝试修改输入、隐藏层和输出层的权重和偏置，观察输出是如何变化的。</li>
</ol>
<h2
id="第五步理解神经网络的训练过程">第五步：理解神经网络的训练过程</h2>
<p>神经网络的训练主要包括两个阶段：<strong>前向传播（Forward
Propagation）</strong> 和
<strong>反向传播（Backpropagation）</strong>。我们来逐步理解这两个过程。</p>
<h4 id="前向传播forward-propagation">1. 前向传播（Forward
Propagation）</h4>
<p>前向传播是将输入数据通过神经网络，计算出预测的输出。</p>
<p><strong>步骤</strong>：</p>
<ol type="1">
<li>输入数据通过输入层传递到隐藏层。</li>
<li>隐藏层的输出传递到输出层。</li>
<li>输出层生成最终的预测输出。</li>
</ol>
<h4 id="损失函数loss-function">2. 损失函数（Loss Function）</h4>
<p>损失函数用于衡量预测输出与真实值之间的差距。一个常见的损失函数是<strong>均方误差（Mean
Squared Error, MSE）</strong>：</p>
<p>$ = _{i=1}^N ( - y_i)^2 $</p>
<p>其中： - $ N $是样本数量。 - $ $是预测值。 - <span
class="math inline">\(y_i\)</span> 是真实值。</p>
<h4 id="反向传播backpropagation">3. 反向传播（Backpropagation）</h4>
<p>反向传播用于调整神经网络中的权重和偏置，以最小化损失函数的值。它基于梯度下降算法，通过计算损失函数相对于每个权重和偏置的偏导数来更新它们。</p>
<p><strong>步骤</strong>： 1. 计算损失函数的梯度。 2.
反向传播这些梯度，通过链式法则调整每一层的权重和偏置。</p>
<h3 id="python-代码实现简化版">Python 代码实现（简化版）</h3>
<p>我们将实现一个简单的神经网络训练过程，使用一个输入样本进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 定义激活函数及其导数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid_derivative</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> - x)<br><br><span class="hljs-comment"># 输入数据和真实输出</span><br>inputs = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br>true_output = np.array([<span class="hljs-number">0.8</span>])<br><br><span class="hljs-comment"># 初始化权重和偏置</span><br>hidden_weights = np.array([[<span class="hljs-number">0.5</span>, -<span class="hljs-number">0.6</span>], <br>                           [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>]])<br>hidden_bias = np.array([<span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>])<br><br>output_weights = np.array([[<span class="hljs-number">0.4</span>, -<span class="hljs-number">0.7</span>]])<br>output_bias = <span class="hljs-number">0.1</span><br><br><span class="hljs-comment"># 学习率</span><br>learning_rate = <span class="hljs-number">0.5</span><br><br><span class="hljs-comment"># 前向传播</span><br>hidden_input = np.dot(hidden_weights, inputs) + hidden_bias<br>hidden_output = sigmoid(hidden_input)<br><br>output_input = np.dot(output_weights, hidden_output) + output_bias<br>predicted_output = sigmoid(output_input)<br><br><span class="hljs-comment"># 计算损失（均方误差）</span><br>loss = np.mean((true_output - predicted_output) ** <span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 反向传播</span><br><span class="hljs-comment"># 输出层误差</span><br>output_error = true_output - predicted_output<br>output_delta = output_error * sigmoid_derivative(predicted_output)<br><br><span class="hljs-comment"># 隐藏层误差</span><br>hidden_error = output_delta.dot(output_weights)<br>hidden_delta = hidden_error * sigmoid_derivative(hidden_output)<br><br><span class="hljs-comment"># 更新权重和偏置</span><br>output_weights += learning_rate * output_delta * hidden_output<br>output_bias += learning_rate * output_delta<br><br>hidden_weights += learning_rate * np.outer(hidden_delta, inputs)<br>hidden_bias += learning_rate * hidden_delta<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;预测输出: <span class="hljs-subst">&#123;predicted_output&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;损失: <span class="hljs-subst">&#123;loss&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;更新后的权重和偏置: <span class="hljs-subst">&#123;output_weights&#125;</span>, <span class="hljs-subst">&#123;output_bias&#125;</span>, <span class="hljs-subst">&#123;hidden_weights&#125;</span>, <span class="hljs-subst">&#123;hidden_bias&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<h4 id="代码解释-2">代码解释：</h4>
<ol type="1">
<li><strong>定义激活函数及其导数</strong>：<code>sigmoid</code> 及
<code>sigmoid_derivative</code>。</li>
<li><strong>初始化输入和真实输出</strong>：<code>inputs</code> 和
<code>true_output</code>。</li>
<li><strong>初始化权重和偏置</strong>：<code>hidden_weights</code>、<code>hidden_bias</code>、<code>output_weights</code>
和 <code>output_bias</code>。</li>
<li><strong>设置学习率</strong>。</li>
<li><strong>前向传播</strong>：计算隐藏层和输出层的输出。</li>
<li><strong>计算损失</strong>：使用均方误差。</li>
<li><strong>反向传播</strong>：计算输出层和隐藏层的误差及更新量。</li>
<li><strong>更新权重和偏置</strong>。</li>
</ol>
<h3 id="练习-4">练习：</h3>
<ol type="1">
<li>在你的 Python 环境中运行上述代码，观察输出和权重更新。</li>
<li>尝试修改输入、真实输出、权重和偏置的初始值，观察训练过程中的变化。</li>
</ol>
<h2 id="第六步扩展神经网络">第六步：扩展神经网络</h2>
<p>我们将扩展神经网络，增加更多的隐藏层和神经元，以处理更复杂的数据。然后，我们会使用批量梯度下降和其他优化技巧。</p>
<h4 id="批量梯度下降batch-gradient-descent">批量梯度下降（Batch Gradient
Descent）</h4>
<p>批量梯度下降是训练神经网络的一个重要方法，它在整个训练数据集上计算损失并更新权重。这种方法有助于稳定训练过程。</p>
<h4 id="扩展网络的步骤">扩展网络的步骤：</h4>
<ol type="1">
<li><strong>增加更多的隐藏层</strong>：可以通过增加更多的隐藏层和神经元来提高网络的表现力。</li>
<li><strong>引入更多激活函数</strong>：比如
ReLU，可以处理更复杂的非线性关系。</li>
<li><strong>使用批量梯度下降</strong>：在整个训练集上计算损失并更新权重。</li>
</ol>
<h3 id="python-代码实现扩展版">Python 代码实现（扩展版）</h3>
<p>以下代码展示了一个包含更多隐藏层的神经网络，并使用批量梯度下降来训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 定义激活函数及其导数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid_derivative</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> - x)<br><br><span class="hljs-comment"># 数据集（简单示例）</span><br>inputs = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.3</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.8</span>]])<br>true_outputs = np.array([[<span class="hljs-number">0.8</span>], [<span class="hljs-number">0.4</span>], [<span class="hljs-number">0.6</span>]])<br><br><span class="hljs-comment"># 初始化权重和偏置</span><br>input_size = <span class="hljs-number">2</span><br>hidden_size1 = <span class="hljs-number">3</span><br>hidden_size2 = <span class="hljs-number">2</span><br>output_size = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># 隐藏层 1</span><br>hidden_weights1 = np.random.rand(hidden_size1, input_size)<br>hidden_bias1 = np.random.rand(hidden_size1)<br><br><span class="hljs-comment"># 隐藏层 2</span><br>hidden_weights2 = np.random.rand(hidden_size2, hidden_size1)<br>hidden_bias2 = np.random.rand(hidden_size2)<br><br><span class="hljs-comment"># 输出层</span><br>output_weights = np.random.rand(output_size, hidden_size2)<br>output_bias = np.random.rand(output_size)<br><br><span class="hljs-comment"># 学习率</span><br>learning_rate = <span class="hljs-number">0.5</span><br><br><span class="hljs-comment"># 训练过程</span><br>epochs = <span class="hljs-number">10000</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-comment"># 前向传播</span><br>    hidden_input1 = np.dot(inputs, hidden_weights1.T) + hidden_bias1<br>    hidden_output1 = sigmoid(hidden_input1)<br><br>    hidden_input2 = np.dot(hidden_output1, hidden_weights2.T) + hidden_bias2<br>    hidden_output2 = sigmoid(hidden_input2)<br><br>    output_input = np.dot(hidden_output2, output_weights.T) + output_bias<br>    predicted_output = sigmoid(output_input)<br><br>    <span class="hljs-comment"># 计算损失</span><br>    loss = np.mean((true_outputs - predicted_output) ** <span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment"># 反向传播</span><br>    <span class="hljs-comment"># 输出层误差</span><br>    output_error = true_outputs - predicted_output<br>    output_delta = output_error * sigmoid_derivative(predicted_output)<br><br>    <span class="hljs-comment"># 隐藏层 2 误差</span><br>    hidden_error2 = np.dot(output_delta, output_weights)<br>    hidden_delta2 = hidden_error2 * sigmoid_derivative(hidden_output2)<br><br>    <span class="hljs-comment"># 隐藏层 1 误差</span><br>    hidden_error1 = np.dot(hidden_delta2, hidden_weights2)<br>    hidden_delta1 = hidden_error1 * sigmoid_derivative(hidden_output1)<br><br>    <span class="hljs-comment"># 更新权重和偏置</span><br>    output_weights += learning_rate * np.dot(output_delta.T, hidden_output2)<br>    output_bias += learning_rate * np.mean(output_delta, axis=<span class="hljs-number">0</span>)<br><br>    hidden_weights2 += learning_rate * np.dot(hidden_delta2.T, hidden_output1)<br>    hidden_bias2 += learning_rate * np.mean(hidden_delta2, axis=<span class="hljs-number">0</span>)<br><br>    hidden_weights1 += learning_rate * np.dot(hidden_delta1.T, inputs)<br>    hidden_bias1 += learning_rate * np.mean(hidden_delta1, axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 打印损失</span><br>    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch&#125;</span>, Loss: <span class="hljs-subst">&#123;loss&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 最终预测输出</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;最终预测输出: <span class="hljs-subst">&#123;predicted_output&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<h4 id="代码解释-3">代码解释：</h4>
<ol type="1">
<li><strong>扩展数据集</strong>：使用多个样本。</li>
<li><strong>增加隐藏层</strong>：增加一个额外的隐藏层
<code>hidden_layer2</code>。</li>
<li><strong>批量梯度下降</strong>：在每个 epoch 中更新权重和偏置。</li>
<li><strong>输出预测结果</strong>：打印损失并显示最终预测输出。</li>
</ol>
<h3 id="练习-5">练习：</h3>
<ol type="1">
<li>请在你的 Python 环境中运行上述代码，并观察输出结果。</li>
<li>尝试调整网络结构（如增加神经元数量或隐藏层），观察对训练过程的影响。</li>
<li>了解批量大小（batch size）和 epoch 数对训练的影响。</li>
</ol>
<h3 id="常见问题">常见问题：</h3>
<ol type="1">
<li><strong>如何避免过拟合？</strong>
<ul>
<li>使用正则化技术（如 L2 正则化）。</li>
<li>使用 Dropout 技术。</li>
<li>增加数据集。</li>
</ul></li>
<li><strong>如何选择学习率？</strong>
<ul>
<li>学习率过高会导致训练不稳定，过低会导致训练过慢。可以使用学习率调度器逐步调整。</li>
</ul></li>
</ol>
<h3 id="true-outputs-超过-1-的情况">True Outputs 超过 1 的情况</h3>
<h4 id="使用-sigmoid-作为输出层激活函数">1. 使用 Sigmoid
作为输出层激活函数</h4>
<p><strong>Sigmoid 函数</strong>的输出范围是 0 到 1。如果
<code>true_outputs</code> 超过 1，会导致以下问题：</p>
<ul>
<li><strong>不匹配的输出范围</strong>：Sigmoid 函数的输出永远不会超过
1，因此 <code>true_outputs</code> 超过 1
会导致较大的误差，可能影响训练效果。</li>
<li><strong>损失函数计算问题</strong>：如果使用均方误差 (MSE)
作为损失函数，过大的误差会导致梯度过大，可能使训练不稳定。</li>
</ul>
<h4 id="解决方法">解决方法：</h4>
<ul>
<li><strong>归一化输出</strong>：将 <code>true_outputs</code> 归一化到 0
到 1 范围内，使之与 Sigmoid 函数的输出匹配。</li>
<li><strong>使用不同的激活函数</strong>：如果输出不应该被限制在 0 到 1
范围，可以考虑其他激活函数，如线性激活函数（即没有激活函数）。</li>
</ul>
<h3 id="修改代码以处理-true-outputs-超过-1">修改代码以处理 True Outputs
超过 1</h3>
<h4 id="方法-1归一化-true-outputs">方法 1：归一化 True Outputs</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设你的 true_outputs 原始范围是 0 到 10</span><br>true_outputs = np.array([[<span class="hljs-number">8</span>], [<span class="hljs-number">4</span>], [<span class="hljs-number">6</span>]])<br><br><span class="hljs-comment"># 归一化 true_outputs 到 0 到 1</span><br>true_outputs = true_outputs / <span class="hljs-number">10.0</span><br></code></pre></td></tr></table></figure>
<h4 id="方法-2使用线性激活函数">方法 2：使用线性激活函数</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 线性激活函数及其导数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">linear</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">linear_derivative</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br><br><span class="hljs-comment"># 修改代码中 sigmoid 函数部分为线性函数</span><br>hidden_output2 = sigmoid(hidden_input2)  <span class="hljs-comment"># 隐藏层仍然使用 Sigmoid</span><br>output_input = np.dot(hidden_output2, output_weights.T) + output_bias<br>predicted_output = linear(output_input)  <span class="hljs-comment"># 输出层使用线性函数</span><br><br><span class="hljs-comment"># 反向传播</span><br>output_error = true_outputs - predicted_output<br>output_delta = output_error * linear_derivative(predicted_output)<br></code></pre></td></tr></table></figure>
<h3
id="示例代码使用线性激活函数的完整示例">示例代码：使用线性激活函数的完整示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 定义激活函数及其导数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid_derivative</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> - x)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">linear</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">linear_derivative</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br><br><span class="hljs-comment"># 数据集（简单示例）</span><br>inputs = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.3</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.8</span>]])<br>true_outputs = np.array([[<span class="hljs-number">8</span>], [<span class="hljs-number">4</span>], [<span class="hljs-number">6</span>]])<br><br><span class="hljs-comment"># 初始化权重和偏置</span><br>input_size = <span class="hljs-number">2</span><br>hidden_size1 = <span class="hljs-number">3</span><br>hidden_size2 = <span class="hljs-number">2</span><br>output_size = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># 隐藏层 1</span><br>hidden_weights1 = np.random.rand(hidden_size1, input_size)<br>hidden_bias1 = np.random.rand(hidden_size1)<br><br><span class="hljs-comment"># 隐藏层 2</span><br>hidden_weights2 = np.random.rand(hidden_size2, hidden_size1)<br>hidden_bias2 = np.random.rand(hidden_size2)<br><br><span class="hljs-comment"># 输出层</span><br>output_weights = np.random.rand(output_size, hidden_size2)<br>output_bias = np.random.rand(output_size)<br><br><span class="hljs-comment"># 学习率</span><br>learning_rate = <span class="hljs-number">0.5</span><br><br><span class="hljs-comment"># 训练过程</span><br>epochs = <span class="hljs-number">10000</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-comment"># 前向传播</span><br>    hidden_input1 = np.dot(inputs, hidden_weights1.T) + hidden_bias1<br>    hidden_output1 = sigmoid(hidden_input1)<br><br>    hidden_input2 = np.dot(hidden_output1, hidden_weights2.T) + hidden_bias2<br>    hidden_output2 = sigmoid(hidden_input2)<br><br>    output_input = np.dot(hidden_output2, output_weights.T) + output_bias<br>    predicted_output = linear(output_input)  <span class="hljs-comment"># 输出层使用线性函数</span><br><br>    <span class="hljs-comment"># 计算损失</span><br>    loss = np.mean((true_outputs - predicted_output) ** <span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment"># 反向传播</span><br>    <span class="hljs-comment"># 输出层误差</span><br>    output_error = true_outputs - predicted_output<br>    output_delta = output_error * linear_derivative(predicted_output)<br><br>    <span class="hljs-comment"># 隐藏层 2 误差</span><br>    hidden_error2 = np.dot(output_delta, output_weights)<br>    hidden_delta2 = hidden_error2 * sigmoid_derivative(hidden_output2)<br><br>    <span class="hljs-comment"># 隐藏层 1 误差</span><br>    hidden_error1 = np.dot(hidden_delta2, hidden_weights2)<br>    hidden_delta1 = hidden_error1 * sigmoid_derivative(hidden_output1)<br><br>    <span class="hljs-comment"># 更新权重和偏置</span><br>    output_weights += learning_rate * np.dot(output_delta.T, hidden_output2)<br>    output_bias += learning_rate * np.mean(output_delta, axis=<span class="hljs-number">0</span>)<br><br>    hidden_weights2 += learning_rate * np.dot(hidden_delta2.T, hidden_output1)<br>    hidden_bias2 += learning_rate * np.mean(hidden_delta2, axis=<span class="hljs-number">0</span>)<br><br>    hidden_weights1 += learning_rate * np.dot(hidden_delta1.T, inputs)<br>    hidden_bias1 += learning_rate * np.mean(hidden_delta1, axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 打印损失</span><br>    <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch&#125;</span>, Loss: <span class="hljs-subst">&#123;loss&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 最终预测输出</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;最终预测输出: <span class="hljs-subst">&#123;predicted_output&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="总结">总结</h3>
<ul>
<li><strong>归一化 <code>true_outputs</code></strong>
是一种简单的处理方法，可以让你的网络输出与 <code>true_outputs</code>
匹配。</li>
<li><strong>使用线性激活函数</strong> 可以适应范围超过 0-1 的
<code>true_outputs</code>，适合于回归问题等情况。</li>
</ul>
<p>你可以根据需要选择合适的处理方法，或者结合两者，确保网络输出与目标范围匹配。如果有进一步的问题或需要更多解释，请随时告知！</p>
<h2 id="各类神经网络在投资中的应用">各类神经网络在投资中的应用</h2>
<p>在投资领域，神经网络和深度学习技术有许多应用，主要用于数据分析、市场预测、交易策略的自动化等。以下是各类神经网络在投资中的具体应用及适用情况：</p>
<ol type="1">
<li><strong>高级优化算法</strong>
<ul>
<li><strong>应用场景</strong>：优化交易策略、调优模型参数。</li>
<li><strong>适用情况</strong>：适用于需要精确控制模型训练过程的场景，如量化交易策略优化。</li>
</ul></li>
<li><strong>深度学习框架</strong>
<ul>
<li><strong>应用场景</strong>：快速构建和部署复杂的模型，如股票价格预测、投资组合优化。</li>
<li><strong>适用情况</strong>：适用于需要构建大规模深度学习模型的任务。</li>
</ul></li>
<li><strong>卷积神经网络（CNN）</strong>
<ul>
<li><strong>应用场景</strong>：分析金融市场中的图像数据，如股票走势图、财经新闻图片。</li>
<li><strong>适用情况</strong>：适用于从图像数据中提取信息，例如技术图表分析，但在投资中应用较少，除非涉及图像或图表识别。</li>
</ul></li>
<li><strong>递归神经网络（RNN）</strong>
<ul>
<li><strong>应用场景</strong>：时间序列预测，如股票价格走势预测、经济指标分析。</li>
<li><strong>适用情况</strong>：非常适合处理金融时间序列数据，能捕捉长期依赖关系。LSTM
和 GRU 是常用的变体。</li>
</ul></li>
<li><strong>正则化技术</strong>
<ul>
<li><strong>应用场景</strong>：防止模型在训练数据上过拟合，提高模型的泛化能力。</li>
<li><strong>适用情况</strong>：适用于所有神经网络模型，以提高其稳定性和性能。</li>
</ul></li>
<li><strong>超参数调优</strong>
<ul>
<li><strong>应用场景</strong>：自动寻找最佳模型参数组合，如调优学习率、层数、神经元数量。</li>
<li><strong>适用情况</strong>：适用于需要自动化调优过程的场景。</li>
</ul></li>
<li><strong>迁移学习</strong>
<ul>
<li><strong>应用场景</strong>：利用预训练模型在金融数据上微调，如利用新闻分类模型来分析财经新闻对市场的影响。</li>
<li><strong>适用情况</strong>：适用于有相关预训练模型且新任务数据有限的情况。</li>
</ul></li>
<li><strong>生成对抗网络（GAN）</strong>
<ul>
<li><strong>应用场景</strong>：生成虚拟的市场数据用于模拟或测试交易策略。</li>
<li><strong>适用情况</strong>：适用于生成新的数据集或增强数据集，但在金融领域应用较少。</li>
</ul></li>
</ol>
<h3 id="最适合投资的神经网络类型">最适合投资的神经网络类型</h3>
<p><strong>递归神经网络（RNN）</strong>及其变体（如 LSTM 和
GRU）是投资领域最常用的神经网络类型，特别适合时间序列预测。这类模型能够处理过去的金融数据并预测未来的市场行为。</p>
<p><strong>深度学习框架</strong>（如 TensorFlow 和
PyTorch）能够帮助你快速构建和部署这些复杂的模型。此外，结合<strong>高级优化算法</strong>和<strong>正则化技术</strong>，可以进一步提高模型的性能和稳定性。</p>
<h3 id="实现步骤">实现步骤</h3>
<h4 id="使用-rnnlstm-进行股票价格预测">使用 RNN/LSTM
进行股票价格预测</h4>
<ol type="1">
<li><strong>数据准备</strong>
<ul>
<li>收集并预处理时间序列数据，如股票价格、交易量等。</li>
<li>常见数据源包括 Yahoo Finance、Alpha Vantage 等。</li>
</ul></li>
<li><strong>模型构建</strong>
<ul>
<li>构建 RNN 或 LSTM 模型，设定输入层、隐藏层和输出层。</li>
<li>定义损失函数（如 MSE）和优化算法（如 Adam）。</li>
</ul></li>
<li><strong>模型训练</strong>
<ul>
<li>使用历史数据训练模型，使用批量梯度下降优化模型参数。</li>
<li>可以设置多次迭代（epochs）和适当的学习率。</li>
</ul></li>
<li><strong>模型评估</strong>
<ul>
<li>在测试数据集上评估模型性能，使用指标如 RMSE、MAE 等。</li>
</ul></li>
<li><strong>预测和应用</strong>
<ul>
<li>使用训练好的模型进行未来价格预测，结合投资策略进行应用。</li>
</ul></li>
</ol>
<h3 id="代码示例">代码示例</h3>
<p>以下是使用 LSTM 进行股票价格预测的简化示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler<br><br><span class="hljs-comment"># 读取数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_stock_data</span>(<span class="hljs-params">file_path</span>):<br>    data = pd.read_csv(file_path, parse_dates=[<span class="hljs-string">&#x27;Date&#x27;</span>], index_col=<span class="hljs-string">&#x27;Date&#x27;</span>)<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># 特征工程：计算技术指标</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate_technical_indicators</span>(<span class="hljs-params">data</span>):<br>    data[<span class="hljs-string">&#x27;SMA&#x27;</span>] = data[<span class="hljs-string">&#x27;close&#x27;</span>].rolling(window=<span class="hljs-number">10</span>).mean()<br>    data[<span class="hljs-string">&#x27;EMA&#x27;</span>] = data[<span class="hljs-string">&#x27;close&#x27;</span>].ewm(span=<span class="hljs-number">10</span>, adjust=<span class="hljs-literal">False</span>).mean()<br><br>    delta = data[<span class="hljs-string">&#x27;close&#x27;</span>].diff()<br>    gain = (delta.where(delta &gt; <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)).rolling(window=<span class="hljs-number">14</span>).mean()<br>    loss = (-delta.where(delta &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)).rolling(window=<span class="hljs-number">14</span>).mean()<br>    rs = gain / loss<br>    data[<span class="hljs-string">&#x27;RSI&#x27;</span>] = <span class="hljs-number">100</span> - (<span class="hljs-number">100</span> / (<span class="hljs-number">1</span> + rs))<br><br>    data[<span class="hljs-string">&#x27;Bollinger Upper&#x27;</span>] = data[<span class="hljs-string">&#x27;SMA&#x27;</span>] + <span class="hljs-number">2</span> * \<br>        data[<span class="hljs-string">&#x27;close&#x27;</span>].rolling(window=<span class="hljs-number">20</span>).std()<br>    data[<span class="hljs-string">&#x27;Bollinger Lower&#x27;</span>] = data[<span class="hljs-string">&#x27;SMA&#x27;</span>] - <span class="hljs-number">2</span> * \<br>        data[<span class="hljs-string">&#x27;close&#x27;</span>].rolling(window=<span class="hljs-number">20</span>).std()<br><br>    data = data.dropna()  <span class="hljs-comment"># 去除 NaN 值</span><br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># 数据预处理：标准化</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_data</span>(<span class="hljs-params">data</span>):<br>    scaler = MinMaxScaler(feature_range=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>    scaled_data = scaler.fit_transform(data)<br>    <span class="hljs-keyword">return</span> scaled_data, scaler<br><br><span class="hljs-comment"># 创建数据集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_dataset</span>(<span class="hljs-params">data, seq_length</span>):<br>    X, y = [], []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(data) - seq_length - <span class="hljs-number">1</span>):<br>        X.append(data[i:(i + seq_length)])<br>        y.append(data[i + seq_length, <span class="hljs-number">3</span>])  <span class="hljs-comment"># 预测 &#x27;Close&#x27; 价格</span><br>    <span class="hljs-keyword">return</span> np.array(X), np.array(y)<br><br><span class="hljs-comment"># LSTM 单元实现</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LSTMCell</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size</span>):<br>        <span class="hljs-variable language_">self</span>.input_size = input_size<br>        <span class="hljs-variable language_">self</span>.hidden_size = hidden_size<br>        <span class="hljs-variable language_">self</span>.Wf = np.random.randn(hidden_size, input_size + hidden_size) * <span class="hljs-number">0.01</span><br>        <span class="hljs-variable language_">self</span>.Wi = np.random.randn(hidden_size, input_size + hidden_size) * <span class="hljs-number">0.01</span><br>        <span class="hljs-variable language_">self</span>.Wc = np.random.randn(hidden_size, input_size + hidden_size) * <span class="hljs-number">0.01</span><br>        <span class="hljs-variable language_">self</span>.Wo = np.random.randn(hidden_size, input_size + hidden_size) * <span class="hljs-number">0.01</span><br>        <span class="hljs-variable language_">self</span>.bf = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br>        <span class="hljs-variable language_">self</span>.bi = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br>        <span class="hljs-variable language_">self</span>.bc = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br>        <span class="hljs-variable language_">self</span>.bo = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, h_prev, c_prev</span>):<br>        combined = np.concatenate((x, h_prev), axis=<span class="hljs-number">0</span>)<br><br>        ft = sigmoid(np.dot(<span class="hljs-variable language_">self</span>.Wf, combined) + <span class="hljs-variable language_">self</span>.bf)<br>        it = sigmoid(np.dot(<span class="hljs-variable language_">self</span>.Wi, combined) + <span class="hljs-variable language_">self</span>.bi)<br>        c_hat = np.tanh(np.dot(<span class="hljs-variable language_">self</span>.Wc, combined) + <span class="hljs-variable language_">self</span>.bc)<br>        c = ft * c_prev + it * c_hat<br>        ot = sigmoid(np.dot(<span class="hljs-variable language_">self</span>.Wo, combined) + <span class="hljs-variable language_">self</span>.bo)<br>        h = ot * np.tanh(c)<br><br>        <span class="hljs-keyword">return</span> h, c<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-comment"># 训练 LSTM</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_lstm</span>(<span class="hljs-params">X, y, hidden_size, epochs, learning_rate</span>):<br>    input_size = X.shape[<span class="hljs-number">2</span>]<br>    lstm_cell = LSTMCell(input_size, hidden_size)<br>    W_out = np.random.randn(<span class="hljs-number">1</span>, hidden_size) * <span class="hljs-number">0.01</span><br>    b_out = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        total_loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">0</span>]):<br>            h_prev = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br>            c_prev = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br>            <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]):<br>                x_t = X[i, t, :].reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>                h_prev, c_prev = lstm_cell.forward(x_t, h_prev, c_prev)<br><br>            y_hat = np.dot(W_out, h_prev) + b_out<br>            loss = np.mean((y_hat - y[i]) ** <span class="hljs-number">2</span>)<br>            total_loss += loss<br><br>            <span class="hljs-comment"># 反向传播</span><br>            dW_out = (y_hat - y[i]) * h_prev.T<br>            db_out = (y_hat - y[i])<br><br>            <span class="hljs-comment"># dh_prev = np.dot(W_out.T, (y_hat - y[i]))</span><br><br>            W_out -= learning_rate * dW_out<br>            b_out -= learning_rate * db_out<br><br>        <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch&#125;</span>, Loss: <span class="hljs-subst">&#123;total_loss / X.shape[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><br>    <span class="hljs-comment"># 返回训练好的模型参数</span><br>    <span class="hljs-keyword">return</span> lstm_cell, W_out, b_out<br><br><span class="hljs-comment"># 测试 LSTM</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_lstm</span>(<span class="hljs-params">X, lstm_cell, W_out, b_out</span>):<br>    h_prev = np.zeros((lstm_cell.hidden_size, <span class="hljs-number">1</span>))<br>    c_prev = np.zeros((lstm_cell.hidden_size, <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]):<br>        x_t = X[t, :].reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        h_prev, c_prev = lstm_cell.forward(x_t, h_prev, c_prev)<br>    y_hat = np.dot(W_out, h_prev) + b_out<br>    <span class="hljs-keyword">return</span> y_hat<br><br><br><span class="hljs-comment"># 主流程</span><br>file_path = <span class="hljs-string">&#x27;data_600519.csv&#x27;</span><br>data = load_stock_data(file_path)<br>data = calculate_technical_indicators(data)<br>scaled_data, scaler = preprocess_data(<br>    data[[<span class="hljs-string">&#x27;open&#x27;</span>, <span class="hljs-string">&#x27;high&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;close&#x27;</span>, <span class="hljs-string">&#x27;volume&#x27;</span>, <span class="hljs-string">&#x27;SMA&#x27;</span>, <span class="hljs-string">&#x27;EMA&#x27;</span>, <span class="hljs-string">&#x27;RSI&#x27;</span>, <span class="hljs-string">&#x27;Bollinger Upper&#x27;</span>, <span class="hljs-string">&#x27;Bollinger Lower&#x27;</span>]])<br>seq_length = <span class="hljs-number">10</span><br>X, y = create_dataset(scaled_data, seq_length)<br><br><span class="hljs-comment"># 训练 LSTM</span><br>hidden_size = <span class="hljs-number">10</span><br>epochs = <span class="hljs-number">500</span><br>learning_rate = <span class="hljs-number">0.5</span><br>lstm_cell, W_out, b_out = train_lstm(X, y, hidden_size, epochs, learning_rate)<br><br><span class="hljs-comment"># 预测</span><br>X_test, y_test = create_dataset(scaled_data[-<span class="hljs-number">20</span>:], seq_length)<br>predicted = [predict_lstm(X_test[i], lstm_cell, W_out, b_out)<br>             <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(X_test))]<br>predicted = np.array(predicted).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># 扩展预测值</span><br>num_features = scaled_data.shape[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 原始数据的特征数量</span><br>predicted_full = np.zeros((predicted.shape[<span class="hljs-number">0</span>], num_features))<br>predicted_full[:, <span class="hljs-number">3</span>] = predicted[:, <span class="hljs-number">0</span>]  <span class="hljs-comment"># 假设 &#x27;Close&#x27; 是第 4 列（索引 3） </span><br><span class="hljs-comment"># 进行逆变换</span><br>original_predicted_full = scaler.inverse_transform(predicted_full)<br>original_predicted = original_predicted_full[:, <span class="hljs-number">3</span>]  <span class="hljs-comment"># 提取 &#x27;Close&#x27; 列</span><br>pingjun = np.mean(original_predicted)<br><br><span class="hljs-comment"># 输出结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;预测结果: <span class="hljs-subst">&#123;original_predicted&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;预测结果(平均值): <span class="hljs-subst">&#123;pingjun&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="总结-1">总结</h3>
<ul>
<li><strong>RNN/LSTM</strong>
非常适合处理时间序列数据，用于投资领域的市场预测和策略制定。</li>
<li>你可以结合<strong>深度学习框架</strong>和<strong>高级优化算法</strong>来提高模型的性能和效率。</li>
<li>结合投资实际需求选择合适的模型和方法，逐步构建和优化你的投资策略。</li>
</ul>
<h3 id="代码逐行解释">代码逐行解释：</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler<br><span class="hljs-comment"># 读取数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_stock_data</span>(<span class="hljs-params">file_path</span>):<br>    data = pd.read_csv(file_path, parse_dates=[<span class="hljs-string">&#x27;Date&#x27;</span>], index_col=<span class="hljs-string">&#x27;Date&#x27;</span>)<br>    <span class="hljs-keyword">return</span> data<br></code></pre></td></tr></table></figure>
<p>这段代码定义了一个名为 <code>load_stock_data</code>
的函数，用于加载和预处理股票数据。该函数接收一个参数
<code>file_path</code>，即包含股票数据的CSV文件的路径。下面是逐行解释：</p>
<ol type="1">
<li><p><code>import numpy as np</code>：这行代码导入了NumPy库，并将其简称为<code>np</code>。NumPy是Python中用于科学计算的一个基础库，提供了大量的数学函数操作以及高性能的多维数组对象。然而，在这段代码中，NumPy库实际上没有被直接使用，可能是为了其他部分的代码（未在此展示）准备的。</p></li>
<li><p><code>import pandas as pd</code>：这行代码导入了Pandas库，并将其简称为<code>pd</code>。Pandas是Python中用于数据分析的一个强大库，提供了易于使用的数据结构和数据分析工具。这段代码主要使用了Pandas来处理CSV格式的股票数据。</p></li>
<li><p><code>from sklearn.preprocessing import MinMaxScaler</code>：这行代码从scikit-learn（一个流行的Python机器学习库）的预处理模块中导入了<code>MinMaxScaler</code>类。<code>MinMaxScaler</code>用于将特征缩放到给定的最小值和最大值之间（通常是0和1），这是数据预处理中常用的一种技术，可以帮助改善许多机器学习算法的性能。然而，在这段特定的代码中，<code>MinMaxScaler</code>没有被直接使用，可能是为了后续的数据处理步骤预留的。</p></li>
<li><p>定义<code>load_stock_data</code>函数：</p>
<ul>
<li><code>def load_stock_data(file_path):</code>：定义了一个名为<code>load_stock_data</code>的函数，它接受一个参数<code>file_path</code>，即包含股票数据的CSV文件的路径。</li>
</ul></li>
<li><p>加载和预处理数据：</p>
<ul>
<li><code>data = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')</code>：使用Pandas的<code>read_csv</code>函数加载CSV文件。<code>parse_dates=['Date']</code>参数告诉Pandas将<code>Date</code>列解析为日期时间类型。<code>index_col='Date'</code>参数指定将<code>Date</code>列用作DataFrame的行索引。这意味着加载后的DataFrame将使用日期时间作为索引，而不是默认的整数索引。</li>
</ul></li>
<li><p>返回处理后的数据：</p>
<ul>
<li><code>return data</code>：函数返回处理后的DataFrame，其中包含了按日期时间索引的股票数据。</li>
</ul></li>
</ol>
<p>综上所述，这段代码的主要作用是加载一个包含股票数据的CSV文件，将<code>Date</code>列解析为日期时间类型，并将其作为行索引，然后返回这个处理后的DataFrame。虽然<code>MinMaxScaler</code>被导入了，但在这段代码中并没有直接使用，可能是为了后续的数据标准化步骤准备的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 特征工程：计算技术指标</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate_technical_indicators</span>(<span class="hljs-params">data</span>):<br>    data[<span class="hljs-string">&#x27;SMA&#x27;</span>] = data[<span class="hljs-string">&#x27;close&#x27;</span>].rolling(window=<span class="hljs-number">10</span>).mean()<br>    data[<span class="hljs-string">&#x27;EMA&#x27;</span>] = data[<span class="hljs-string">&#x27;close&#x27;</span>].ewm(span=<span class="hljs-number">10</span>, adjust=<span class="hljs-literal">False</span>).mean()<br><br>    delta = data[<span class="hljs-string">&#x27;close&#x27;</span>].diff()<br>    gain = (delta.where(delta &gt; <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)).rolling(window=<span class="hljs-number">14</span>).mean()<br>    loss = (-delta.where(delta &lt; <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)).rolling(window=<span class="hljs-number">14</span>).mean()<br>    rs = gain / loss<br>    data[<span class="hljs-string">&#x27;RSI&#x27;</span>] = <span class="hljs-number">100</span> - (<span class="hljs-number">100</span> / (<span class="hljs-number">1</span> + rs))<br><br>    data[<span class="hljs-string">&#x27;Bollinger Upper&#x27;</span>] = data[<span class="hljs-string">&#x27;SMA&#x27;</span>] + <span class="hljs-number">2</span> * \<br>        data[<span class="hljs-string">&#x27;close&#x27;</span>].rolling(window=<span class="hljs-number">20</span>).std()<br>    data[<span class="hljs-string">&#x27;Bollinger Lower&#x27;</span>] = data[<span class="hljs-string">&#x27;SMA&#x27;</span>] - <span class="hljs-number">2</span> * \<br>        data[<span class="hljs-string">&#x27;close&#x27;</span>].rolling(window=<span class="hljs-number">20</span>).std()<br><br>    data = data.dropna()  <span class="hljs-comment"># 去除 NaN 值</span><br>    <span class="hljs-keyword">return</span> data<br></code></pre></td></tr></table></figure>
<p>这段代码定义了一个名为 <code>calculate_technical_indicators</code>
的函数，它接受一个包含股票价格数据的
<code>DataFrame</code>（假设至少包含 <code>close</code>
列，即收盘价）作为输入，并计算并添加几个常见的技术指标到该
<code>DataFrame</code> 中，最后返回更新后的
<code>DataFrame</code>。这些技术指标包括简单移动平均（SMA）、指数移动平均（EMA）、相对强弱指数（RSI）和布林带（Bollinger
Bands）。下面是对每个步骤的详细解释：</p>
<ol type="1">
<li><strong>简单移动平均（SMA）</strong>：
<ul>
<li>使用 <code>rolling(window=10).mean()</code>
计算过去10天的收盘价的平均值，结果存储在 <code>data['SMA']</code>
中。</li>
</ul></li>
<li><strong>指数移动平均（EMA）</strong>：
<ul>
<li>使用 <code>ewm(span=10, adjust=False).mean()</code>
计算指数移动平均。这里的 <code>span=10</code>
大致相当于传统的平滑系数（alpha）的倒数，但 <code>ewm</code> 方法通过
<code>span</code> 参数自动计算平滑系数。<code>adjust=False</code>
表示不使用调整因子（即不将EMA的初始值设为第一个值）。结果存储在
<code>data['EMA']</code> 中。</li>
</ul></li>
<li><strong>相对强弱指数（RSI）</strong>：
<ul>
<li>首先计算收盘价的差异（<code>delta</code>），即当天的收盘价与前一天的收盘价之差。</li>
<li>然后分别计算过去14天内所有上涨日（<code>gain</code>）和下跌日（<code>loss</code>）的平均值。上涨日定义为
<code>delta &gt; 0</code> 的日子，下跌日定义为 <code>delta &lt; 0</code>
的日子。注意，这里使用了 <code>where</code>
方法来将非上涨/下跌日的值设为0，以便计算平均值。</li>
<li>计算RS（相对强弱），即上涨平均除以下跌平均。</li>
<li>最后，使用RS值计算RSI（相对强弱指数），公式为
<code>100 - (100 / (1 + rs))</code>。结果存储在 <code>data['RSI']</code>
中。</li>
</ul></li>
<li><strong>布林带（Bollinger Bands）</strong>：
<ul>
<li>计算过去20天的收盘价的标准差（<code>std()</code>），并以此为基础计算布林带的上限和下限。</li>
<li>布林带上限是简单移动平均（SMA）加上两倍的标准差，存储在
<code>data['Bollinger Upper']</code> 中。</li>
<li>布林带下限是简单移动平均（SMA）减去两倍的标准差，存储在
<code>data['Bollinger Lower']</code> 中。</li>
</ul></li>
<li><strong>去除 NaN 值</strong>：
<ul>
<li>使用 <code>dropna()</code>
方法去除所有包含NaN值的行。这是必要的，因为计算移动平均、标准差等统计量时，在数据集的开始部分会产生NaN值。</li>
</ul></li>
<li><strong>返回更新后的 DataFrame</strong>：
<ul>
<li>函数返回更新后包含新计算的技术指标的 <code>DataFrame</code>。</li>
</ul></li>
</ol>
<p>这个函数是金融数据分析中常用的一个工具，可以帮助投资者和分析师更好地理解股票价格的动态和趋势。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数据预处理：标准化</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_data</span>(<span class="hljs-params">data</span>):<br>    scaler = MinMaxScaler(feature_range=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>    scaled_data = scaler.fit_transform(data)<br>    <span class="hljs-keyword">return</span> scaled_data, scaler<br></code></pre></td></tr></table></figure>
<p>这个函数 <code>preprocess_data</code> 的目的是对给定的数据集
<code>data</code> 进行预处理，具体来说是进行特征缩放（Feature
Scaling），将数据的特征值缩放到一个指定的范围内，这里使用的是 0 到 1
的范围。这种预处理步骤对于许多机器学习算法来说是非常重要的，因为它可以帮助改善算法的收敛速度和性能。</p>
<p>让我们逐行解释这个函数：</p>
<ol type="1">
<li><p><code>scaler = MinMaxScaler(feature_range=(0, 1))</code></p>
<p>这行代码创建了一个 <code>MinMaxScaler</code> 对象，它是从
<code>sklearn.preprocessing</code>
模块中导入的。<code>MinMaxScaler</code>
将数据缩放到给定的最小值和最大值之间（这里是 0 和
1）。默认情况下，<code>MinMaxScaler</code> 会将数据缩放到 [0,
1]，但通过设置 <code>feature_range</code>
参数，我们可以自定义这个范围。在这个例子中，我们显式地指定了
<code>feature_range=(0, 1)</code>，尽管这是默认值。</p></li>
<li><p><code>scaled_data = scaler.fit_transform(data)</code></p>
<p>这行代码执行了两个步骤：首先，<code>fit</code>
方法计算了数据的最小值和最大值，这是进行缩放所必需的；然后，<code>transform</code>
方法使用这些计算出的最小值和最大值将原始数据 <code>data</code>
缩放到指定的范围内（即 0 到 1）。这两个步骤通过
<code>fit_transform</code>
方法合并为一个步骤，以提高效率。<code>scaled_data</code>
变量现在包含了缩放后的数据。</p></li>
<li><p><code>return scaled_data, scaler</code></p>
<p>最后，函数返回两个值：缩放后的数据 <code>scaled_data</code> 和
<code>MinMaxScaler</code> 对象 <code>scaler</code>。返回
<code>scaler</code>
对象可能很有用，因为它允许我们在未来的数据点（比如测试集或新数据）上使用相同的缩放参数进行缩放，以确保数据的一致性和可比性。</p></li>
</ol>
<p>总的来说，这个函数是一个用于数据预处理的实用工具，它通过特征缩放将数据集的特征值缩放到
0 到 1 的范围内，并返回缩放后的数据以及用于缩放的
<code>MinMaxScaler</code> 对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建数据集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_dataset</span>(<span class="hljs-params">data, seq_length</span>):<br>    X, y = [], []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(data) - seq_length - <span class="hljs-number">1</span>):<br>        X.append(data[i:(i + seq_length)])<br>        y.append(data[i + seq_length, <span class="hljs-number">3</span>])  <span class="hljs-comment"># 预测 &#x27;Close&#x27; 价格</span><br>    <span class="hljs-keyword">return</span> np.array(X), np.array(y)<br></code></pre></td></tr></table></figure>
<p>这个函数 <code>create_dataset</code> 的目的是从给定的数据
<code>data</code>
中创建一个用于监督学习（特别是时间序列预测）的数据集。它接收两个参数：<code>data</code>
和 <code>seq_length</code>。<code>data</code>
是一个多维数组，其中包含了时间序列数据；<code>seq_length</code>
是一个整数，指定了每个输入序列的长度。函数返回两个数组：<code>X</code>
和 <code>y</code>，其中 <code>X</code> 包含输入序列，<code>y</code>
包含每个输入序列对应的目标值（即下一个时间步的某个特定特征值）。</p>
<p>不过，需要注意的是，函数中存在一个潜在的错误或不一致之处，这取决于
<code>data</code>
的具体结构。下面是对函数行为的详细解释和潜在问题的说明：</p>
<ol type="1">
<li><p><strong>初始化 X 和 y</strong>：函数开始时，通过空列表
<code>X</code> 和 <code>y</code> 来存储输入序列和目标值。</p></li>
<li><p><strong>循环遍历数据</strong>：函数通过一个循环遍历
<code>data</code>，从索引 <code>i</code> 开始，直到
<code>len(data) - seq_length - 1</code>。这是因为我们需要有足够的后续数据来作为目标值（即
<code>y</code>），同时保持每个输入序列的长度为
<code>seq_length</code>。</p></li>
<li><p><strong>构建输入序列 X</strong>：在每次循环中，通过
<code>data[i:(i + seq_length)]</code> 从 <code>data</code>
中切取一个长度为 <code>seq_length</code> 的序列，并将其添加到
<code>X</code> 列表中。</p></li>
<li><p><strong>构建目标值 y</strong>：这里有一个潜在的问题。代码
<code>y.append(data[i + seq_length, 3])</code> 试图从
<code>data[i + seq_length]</code> 中获取第四个元素（索引为 3，因为索引从
0 开始）。但是，这假设 <code>data</code>
是一个二维数组，并且每一行都至少有四个元素。如果 <code>data</code>
是一维数组或每行的元素数量少于四个，这将导致错误。此外，如果
<code>data</code> 是三维或更高维的，这种索引方式也是不正确的。</p></li>
<li><p><strong>返回结果</strong>：最后，函数将 <code>X</code> 和
<code>y</code> 列表转换为 NumPy 数组并返回它们。</p></li>
</ol>
<p><strong>修正建议</strong>：</p>
<ul>
<li><p>如果 <code>data</code>
是一维时间序列数据，并且你想要预测的是序列中下一个时间步的某个特定值（但不一定是第四个值），你应该首先确认这一点，并在代码中明确这一点。例如，如果你想要预测的是下一个时间步的值，那么你可以将
<code>y.append(data[i + seq_length])</code> 改为
<code>y.append(data[i + seq_length])</code>（但这会假设你预测的是整个序列的下一个值，而不是某个特定特征）。</p></li>
<li><p>如果 <code>data</code>
是二维或多维的，并且你确实想要获取每个序列的下一个时间步的第四个元素作为目标值，你需要确保
<code>data</code> 的形状和结构符合这种索引方式。</p></li>
<li><p>如果 <code>data</code>
的形状和结构不确定，你可能需要添加一些检查来验证这些假设，或者在文档中清楚地说明函数的使用条件和要求。</p></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># LSTM 单元实现</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LSTMCell</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size</span>):<br>        <span class="hljs-variable language_">self</span>.input_size = input_size<br>        <span class="hljs-variable language_">self</span>.hidden_size = hidden_size<br>        <span class="hljs-variable language_">self</span>.Wf = np.random.randn(hidden_size, input_size + hidden_size) * <span class="hljs-number">0.01</span><br>        <span class="hljs-variable language_">self</span>.Wi = np.random.randn(hidden_size, input_size + hidden_size) * <span class="hljs-number">0.01</span><br>        <span class="hljs-variable language_">self</span>.Wc = np.random.randn(hidden_size, input_size + hidden_size) * <span class="hljs-number">0.01</span><br>        <span class="hljs-variable language_">self</span>.Wo = np.random.randn(hidden_size, input_size + hidden_size) * <span class="hljs-number">0.01</span><br>        <span class="hljs-variable language_">self</span>.bf = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br>        <span class="hljs-variable language_">self</span>.bi = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br>        <span class="hljs-variable language_">self</span>.bc = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br>        <span class="hljs-variable language_">self</span>.bo = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, h_prev, c_prev</span>):<br>        <span class="hljs-comment"># # 假设 x 和 h_prev 都是一维数组，转换为二维以便拼接</span><br>        <span class="hljs-comment"># x = x.reshape(-1, 1)</span><br>        <span class="hljs-comment"># h_prev = h_prev.reshape(-1, 1)</span><br>        combined = np.concatenate((x, h_prev), axis=<span class="hljs-number">0</span>)<br><br>        ft = sigmoid(np.dot(<span class="hljs-variable language_">self</span>.Wf, combined) + <span class="hljs-variable language_">self</span>.bf)<br>        it = sigmoid(np.dot(<span class="hljs-variable language_">self</span>.Wi, combined) + <span class="hljs-variable language_">self</span>.bi)<br>        c_hat = np.tanh(np.dot(<span class="hljs-variable language_">self</span>.Wc, combined) + <span class="hljs-variable language_">self</span>.bc)<br>        c = ft * c_prev + it * c_hat<br>        ot = sigmoid(np.dot(<span class="hljs-variable language_">self</span>.Wo, combined) + <span class="hljs-variable language_">self</span>.bo)<br>        h = ot * np.tanh(c)<br><br>        <span class="hljs-keyword">return</span> h, c<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br></code></pre></td></tr></table></figure>
<p>您提供的 <code>LSTMCell</code> 类实现了一个基本的 LSTM (长短期记忆)
单元的前向传播过程。LSTM 是一种特殊的 RNN (循环神经网络)
单元，它通过引入“门”机制（遗忘门、输入门、输出门）来解决传统 RNN
在处理长序列时容易出现的梯度消失或梯度爆炸问题。下面是对您实现的详细解释和可能需要注意的地方：</p>
<h4 id="初始化">初始化</h4>
<p>在 <code>__init__</code> 方法中，您为 LSTM
单元的权重和偏置项进行了随机初始化，并将它们乘以一个小的系数（这里是
0.01）来减少初始化时的随机性对训练的影响。这是深度学习中的常见做法，称为权重初始化。</p>
<h4 id="前向传播">前向传播</h4>
<p>在 <code>forward</code> 方法中，您实现了 LSTM
的前向传播过程，这包括计算遗忘门 <code>ft</code>、输入门
<code>it</code>、候选单元状态 <code>c_hat</code>、更新后的单元状态
<code>c</code> 和输出门 <code>ot</code>，最后输出隐藏状态
<code>h</code>。</p>
<h4 id="注意事项">注意事项</h4>
<ol type="1">
<li><p><strong>维度匹配</strong>：在
<code>np.concatenate((x, h_prev), axis=0)</code>
中，您沿着第一个轴（axis=0）拼接了输入 <code>x</code> 和前一个隐藏状态
<code>h_prev</code>。然而，在 LSTM 中，通常 <code>x</code> 和
<code>h_prev</code> 的维度应该是 <code>(input_size,)</code> 和
<code>(hidden_size,)</code>，并且它们应该沿着特征维度（即第二维，axis=1）进行拼接。因此，正确的拼接方式应该是
<code>np.concatenate((x.reshape(-1, 1), h_prev.reshape(-1, 1)), axis=1)</code>，但更常见的是直接使用
<code>np.hstack((x, h_prev))</code> 或者假设 <code>x</code> 和
<code>h_prev</code> 已经是二维的，并直接拼接
<code>np.concatenate((x, h_prev), axis=1)</code>。</p></li>
<li><p><strong>广播机制</strong>：在
<code>ft * c_prev + it * c_hat</code> 和 <code>ot * np.tanh(c)</code>
中，NumPy
的广播机制会自动处理不同形状的数组相乘。但是，为了避免潜在的维度问题，确保
<code>ft</code>, <code>it</code>, <code>c_prev</code>,
<code>c_hat</code>, <code>ot</code>, 和 <code>c</code>
的形状符合预期是很重要的。</p></li>
<li><p><strong>性能优化</strong>：虽然您的实现是教育和研究目的的好例子，但在实际应用中，使用专门的深度学习库（如
TensorFlow 或 PyTorch）会更高效，因为这些库经过优化，可以在 GPU
上并行处理，并提供自动微分等功能。</p></li>
</ol>
<h4 id="示例修正">示例修正</h4>
<p>假设 <code>x</code> 和 <code>h_prev</code>
都是一维数组（或需要被转换为二维数组以便拼接），这里是一个简化的修正版本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, h_prev, c_prev</span>):<br>    <span class="hljs-comment"># 假设 x 和 h_prev 都是一维数组，转换为二维以便拼接</span><br>    x = x.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>    h_prev = h_prev.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>    combined = np.hstack((x, h_prev))<br><br>    <span class="hljs-comment"># ...（接下来的计算保持不变）</span><br><br>    <span class="hljs-keyword">return</span> h, c<br></code></pre></td></tr></table></figure>
<p>注意，这里假设 <code>x</code> 和 <code>h_prev</code>
都是标量或一维数组，并通过 <code>.reshape(-1, 1)</code>
转换为二维数组，然后使用 <code>np.hstack</code>
沿着第二个轴（特征轴）进行拼接。如果它们已经是二维的，则可以直接使用
<code>np.concatenate</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 训练 LSTM</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_lstm</span>(<span class="hljs-params">X, y, hidden_size, epochs, learning_rate</span>):<br>    input_size = X.shape[<span class="hljs-number">2</span>]<br>    lstm_cell = LSTMCell(input_size, hidden_size)<br>    W_out = np.random.randn(<span class="hljs-number">1</span>, hidden_size) * <span class="hljs-number">0.01</span><br>    b_out = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        total_loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">0</span>]):<br>            h_prev = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br>            c_prev = np.zeros((hidden_size, <span class="hljs-number">1</span>))<br>            <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]):<br>                x_t = X[i, t, :].reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>                h_prev, c_prev = lstm_cell.forward(x_t, h_prev, c_prev)<br><br>            y_hat = np.dot(W_out, h_prev) + b_out<br>            loss = np.mean((y_hat - y[i]) ** <span class="hljs-number">2</span>)<br>            total_loss += loss<br><br>            <span class="hljs-comment"># 反向传播</span><br>            dW_out = (y_hat - y[i]) * h_prev.T<br>            db_out = (y_hat - y[i])<br><br>            <span class="hljs-comment"># dh_prev = np.dot(W_out.T, (y_hat - y[i]))</span><br><br>            W_out -= learning_rate * dW_out<br>            b_out -= learning_rate * db_out<br><br>        <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch&#125;</span>, Loss: <span class="hljs-subst">&#123;total_loss / X.shape[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><br>    <span class="hljs-comment"># 返回训练好的模型参数</span><br>    <span class="hljs-keyword">return</span> lstm_cell, W_out, b_out<br></code></pre></td></tr></table></figure>
<p>您提供的 <code>train_lstm</code> 函数实现了使用单个 LSTM
单元进行序列数据训练的基本框架，但是有几个关键的问题和遗漏点需要注意和修正：</p>
<ol type="1">
<li><p><strong>反向传播不完整</strong>：
您的代码只实现了输出层的权重和偏置的更新，但没有实现 LSTM
单元内部权重（<code>Wf</code>, <code>Wi</code>, <code>Wc</code>,
<code>Wo</code> 和对应的偏置 <code>bf</code>, <code>bi</code>,
<code>bc</code>, <code>bo</code>）的更新。这意味着 LSTM
单元的参数在训练过程中保持不变，这显然是不正确的。</p></li>
<li><p><strong>损失函数和梯度计算</strong>：
您的损失函数是均方误差（MSE），这是合理的。但是，梯度计算
<code>dW_out</code> 和 <code>db_out</code>
的方式在批量处理时可能不够准确，因为它们只考虑了最后一个时间步的隐藏状态。在序列预测中，通常需要考虑整个序列的损失。</p></li>
<li><p><strong>LSTM 单元内部更新</strong>： 为了更新 LSTM
单元内部的权重，您需要实现 LSTM 的反向传播算法（也称为通过时间反向传播
BPTT）。这涉及到计算每个时间步的误差项，并将它们反向传播回 LSTM
单元以更新权重。</p></li>
<li><p><strong>学习率和优化器</strong>：
您使用了简单的梯度下降（SGD）来更新参数，这是可行的，但在实际应用中，更复杂的优化器（如
Adam、RMSprop 等）可能更有效。</p></li>
<li><p><strong>序列长度和批量处理</strong>：
您的代码目前是按单个序列进行训练的，没有实现批量处理。在实际应用中，为了提高效率，通常会同时处理多个序列。</p></li>
</ol>
<p>由于实现 LSTM
的完整反向传播算法相当复杂，这里不会给出完整的代码，但我可以提供一些指导思路：</p>
<ul>
<li><strong>实现 LSTM
的反向传播</strong>：您需要计算每个时间步的误差项，并将其反向传播回 LSTM
单元。这涉及到计算关于遗忘门、输入门、候选单元状态和输出门的梯度。</li>
<li><strong>更新 LSTM 权重</strong>：使用反向传播得到的梯度来更新 LSTM
单元的权重和偏置。</li>
<li><strong>考虑序列和批量处理</strong>：在循环中同时迭代多个序列和每个序列的多个时间步。</li>
</ul>
<p>如果您正在学习 LSTM 并希望逐步构建理解，我建议从更简单的 RNN
单元开始，并逐步增加复杂性，直到您能够处理 LSTM。同时，使用像 TensorFlow
或 PyTorch
这样的深度学习框架可以大大简化这些过程，因为它们已经为您处理了大部分底层细节。</p>
<p>对于您当前的项目，如果您只是想快速了解 LSTM
的基本工作原理，并且不打算深入实现其反向传播算法，您可以使用这些框架来构建和训练
LSTM 模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 测试 LSTM</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_lstm</span>(<span class="hljs-params">X, lstm_cell, W_out, b_out</span>):<br>    h_prev = np.zeros((lstm_cell.hidden_size, <span class="hljs-number">1</span>))<br>    c_prev = np.zeros((lstm_cell.hidden_size, <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]):<br>        x_t = X[t, :].reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        h_prev, c_prev = lstm_cell.forward(x_t, h_prev, c_prev)<br>    y_hat = np.dot(W_out, h_prev) + b_out<br>    <span class="hljs-keyword">return</span> y_hat<br></code></pre></td></tr></table></figure>
<p>您提供的 <code>predict_lstm</code> 函数实现了使用训练好的 LSTM
单元进行序列预测的过程。这个函数接收输入序列 <code>X</code>、训练好的
LSTM 单元 <code>lstm_cell</code>、输出层的权重 <code>W_out</code> 和偏置
<code>b_out</code>，并返回预测的输出
<code>y_hat</code>。不过，有几个小地方可能需要调整或注意：</p>
<ol type="1">
<li><p><strong>输入序列的形状</strong>： 在您的函数中，您假设了
<code>X</code> 的形状是
<code>(time_steps, input_size)</code>，即它是一个二维数组，其中第一维是时间步长，第二维是每个时间步的输入特征数量。这通常是合理的，但请确保这与您训练
LSTM 时使用的输入序列形状相匹配。</p></li>
<li><p><strong>输出形状</strong>： <code>y_hat</code> 的形状将是
<code>(1, 1)</code>，因为它是一个单个预测值（假设 <code>W_out</code>
的形状是
<code>(1, hidden_size)</code>）。如果您的任务是多类别分类或需要多个输出值，您需要相应地调整
<code>W_out</code> 的形状。</p></li>
<li><p><strong>广播机制</strong>： 在
<code>np.dot(W_out, h_prev) + b_out</code> 中，NumPy 的广播机制将确保
<code>b_out</code>（形状 <code>(1, 1)</code>）被正确地添加到
<code>np.dot(W_out, h_prev)</code> 的结果中。但是，请确保
<code>W_out</code> 和 <code>h_prev</code> 的形状是兼容的。</p></li>
<li><p><strong>错误处理</strong>：
您的函数没有错误处理机制。在实际应用中，添加一些基本的错误检查（如检查输入形状、权重和偏置的形状等）是一个好习惯。</p></li>
<li><p><strong>批量预测</strong>：
当前函数只能对单个序列进行预测。如果您需要同时预测多个序列，您需要将这个函数包装在一个循环或函数中，以便能够迭代多个输入序列。</p></li>
<li><p><strong>函数注释</strong>：
添加函数注释可以帮助其他开发者（或未来的您）更好地理解函数的用途、参数和返回值。</p></li>
</ol>
<p>这里是一个添加了简单注释的 <code>predict_lstm</code> 函数示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_lstm</span>(<span class="hljs-params">X, lstm_cell, W_out, b_out</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    使用训练好的 LSTM 单元进行序列预测。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">    - X: 输入序列，形状为 (time_steps, input_size)</span><br><span class="hljs-string">    - lstm_cell: 训练好的 LSTMCell 实例</span><br><span class="hljs-string">    - W_out: 输出层的权重矩阵，形状为 (num_outputs, hidden_size)</span><br><span class="hljs-string">    - b_out: 输出层的偏置向量，形状为 (num_outputs, 1)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">    - y_hat: 预测的输出值，形状为 (num_outputs, 1)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    h_prev = np.zeros((lstm_cell.hidden_size, <span class="hljs-number">1</span>))<br>    c_prev = np.zeros((lstm_cell.hidden_size, <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">0</span>]):  <span class="hljs-comment"># 注意这里使用 X.shape[0] 遍历时间步</span><br>        x_t = X[t, :].reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        h_prev, c_prev = lstm_cell.forward(x_t, h_prev, c_prev)<br>    y_hat = np.dot(W_out, h_prev) + b_out<br>    <span class="hljs-keyword">return</span> y_hat<br></code></pre></td></tr></table></figure>
<p>请注意，我在遍历时间步时使用了 <code>X.shape[0]</code> 而不是
<code>X.shape[1]</code>，因为 <code>X</code> 的形状是
<code>(time_steps, input_size)</code>，所以时间步数应该是第一维。如果您的
<code>X</code> 形状实际上是
<code>(batch_size, time_steps, input_size)</code>（即包含批量处理），则您需要进一步修改这个函数以处理批量数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 主流程</span><br>file_path = <span class="hljs-string">&#x27;data_600519.csv&#x27;</span><br>data = load_stock_data(file_path)<br>data = calculate_technical_indicators(data)<br>scaled_data, scaler = preprocess_data(<br>    data[[<span class="hljs-string">&#x27;open&#x27;</span>, <span class="hljs-string">&#x27;high&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;close&#x27;</span>, <span class="hljs-string">&#x27;volume&#x27;</span>, <span class="hljs-string">&#x27;SMA&#x27;</span>, <span class="hljs-string">&#x27;EMA&#x27;</span>, <span class="hljs-string">&#x27;RSI&#x27;</span>, <span class="hljs-string">&#x27;Bollinger Upper&#x27;</span>, <span class="hljs-string">&#x27;Bollinger Lower&#x27;</span>]])<br>seq_length = <span class="hljs-number">10</span><br>X, y = create_dataset(scaled_data, seq_length)<br></code></pre></td></tr></table></figure>
<p>下面是对您提供的代码段进行逐行解释，包括注释：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义股票数据文件的路径</span><br>file_path = <span class="hljs-string">&#x27;data_600519.csv&#x27;</span><br><br><span class="hljs-comment"># 加载股票数据文件，这里假设load_stock_data是一个自定义函数，用于从CSV文件中读取并处理股票数据</span><br><span class="hljs-comment"># 函数可能返回Pandas DataFrame对象，其中包含股票的各种数据（如开盘价、收盘价等）</span><br>data = load_stock_data(file_path)<br><br><span class="hljs-comment"># 计算技术指标，这里假设calculate_technical_indicators是一个自定义函数</span><br><span class="hljs-comment"># 它接收股票数据DataFrame作为输入，并计算一些技术指标（如简单移动平均SMA、指数移动平均EMA、相对强弱指数RSI、布林带等）</span><br><span class="hljs-comment"># 然后将这些指标作为新列添加到原始DataFrame中，并返回更新后的DataFrame</span><br>data = calculate_technical_indicators(data)<br><br><span class="hljs-comment"># 数据预处理，这里假设preprocess_data是一个自定义函数</span><br><span class="hljs-comment"># 它接收包含选定特征（如开盘价、收盘价、成交量、计算出的技术指标等）的DataFrame作为输入</span><br><span class="hljs-comment"># 函数首先对数据进行缩放（可能是归一化或标准化），以便模型能够更好地学习</span><br><span class="hljs-comment"># 然后返回缩放后的数据以及用于缩放数据的scaler对象（这可能在后续的数据预测中需要用到）</span><br>scaled_data, scaler = preprocess_data(<br>    data[[<span class="hljs-string">&#x27;open&#x27;</span>, <span class="hljs-string">&#x27;high&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;close&#x27;</span>, <span class="hljs-string">&#x27;volume&#x27;</span>, <span class="hljs-string">&#x27;SMA&#x27;</span>, <span class="hljs-string">&#x27;EMA&#x27;</span>, <span class="hljs-string">&#x27;RSI&#x27;</span>, <span class="hljs-string">&#x27;Bollinger Upper&#x27;</span>, <span class="hljs-string">&#x27;Bollinger Lower&#x27;</span>]]<br>)<br><br><span class="hljs-comment"># 定义序列长度，这是时间序列分析中常用的一个参数</span><br><span class="hljs-comment"># 它决定了模型在预测时考虑的历史数据点的数量</span><br>seq_length = <span class="hljs-number">10</span><br><br><span class="hljs-comment"># 创建数据集，这里假设create_dataset是一个自定义函数</span><br><span class="hljs-comment"># 它接收缩放后的数据和序列长度作为输入</span><br><span class="hljs-comment"># 函数的作用是将缩放后的数据转换为适合模型训练的格式</span><br><span class="hljs-comment"># 通常，这意味着将时间序列数据转换为监督学习问题所需的输入（X）和目标（y）</span><br><span class="hljs-comment"># 在时间序列预测中，X通常是多个连续的时间步，而y是紧随其后的时间步的某个值（如下一个时间步的收盘价）</span><br>X, y = create_dataset(scaled_data, seq_length)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 训练 LSTM</span><br>hidden_size = <span class="hljs-number">10</span><br>epochs = <span class="hljs-number">500</span><br>learning_rate = <span class="hljs-number">0.5</span><br>lstm_cell, W_out, b_out = train_lstm(X, y, hidden_size, epochs, learning_rate)<br><br><span class="hljs-comment"># 预测</span><br>X_test, y_test = create_dataset(scaled_data[-<span class="hljs-number">20</span>:], seq_length)<br>predicted = [predict_lstm(X_test[i], lstm_cell, W_out, b_out)<br>             <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(X_test))]<br>predicted = np.array(predicted).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># 扩展预测值</span><br>num_features = scaled_data.shape[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 原始数据的特征数量</span><br>predicted_full = np.zeros((predicted.shape[<span class="hljs-number">0</span>], num_features))<br>predicted_full[:, <span class="hljs-number">3</span>] = predicted[:, <span class="hljs-number">0</span>]  <span class="hljs-comment"># 假设 &#x27;Close&#x27; 是第 4 列（索引 3）</span><br><span class="hljs-comment"># 进行逆变换</span><br>original_predicted_full = scaler.inverse_transform(predicted_full)<br>original_predicted = original_predicted_full[:, <span class="hljs-number">3</span>]  <span class="hljs-comment"># 提取 &#x27;Close&#x27; 列</span><br>pingjun = np.mean(original_predicted)<br><br><span class="hljs-comment"># 输出结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;预测结果: <span class="hljs-subst">&#123;original_predicted&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;预测结果(平均值): <span class="hljs-subst">&#123;pingjun&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>在您提供的代码段中，您首先训练了一个LSTM模型，并使用该模型进行了预测。然而，有几个地方需要注意或可能需要修改，以确保代码能够正确运行并产生预期的结果。以下是对代码段的详细解释和可能的修改建议：</p>
<ol type="1">
<li><p><strong>训练 LSTM</strong>： 您假设有一个名为
<code>train_lstm</code> 的函数，它接收输入数据 <code>X</code>、目标数据
<code>y</code>、隐藏层大小 <code>hidden_size</code>、训练轮次
<code>epochs</code> 和学习率 <code>learning_rate</code>，并返回训练好的
LSTM 单元、输出层权重 <code>W_out</code> 和偏置
<code>b_out</code>。这个函数的具体实现不在您提供的代码段中，但它是关键部分，需要确保它正确实现了
LSTM 的训练过程。</p></li>
<li><p><strong>预测</strong>：</p>
<ul>
<li>您使用 <code>create_dataset</code> 函数从 <code>scaled_data</code>
的最后 20 个时间点中创建了一个测试数据集 <code>X_test</code> 和
<code>y_test</code>。请注意，由于您正在使用 LSTM
进行预测，因此实际上可能不需要
<code>y_test</code>（除非您打算在测试集上评估模型性能）。</li>
<li>您定义了一个 <code>predict_lstm</code>
函数（尽管它的实现细节不在这里），该函数应该接收单个时间步的输入序列
<code>X_test[i]</code>、LSTM
单元、输出层权重和偏置，并返回预测结果。然而，在 LSTM
的上下文中，通常一次预测多个时间步（使用滑动窗口或类似方法），但这里您似乎是对每个时间步单独进行预测。</li>
<li>您将预测结果收集到一个列表中，并将其转换为 NumPy
数组。然后，您创建了一个与原始数据特征数量相同大小的零数组
<code>predicted_full</code>，并将预测结果（假设是收盘价）放入该数组的适当列中。</li>
</ul></li>
<li><p><strong>逆变换和结果提取</strong>：</p>
<ul>
<li>您使用 <code>scaler.inverse_transform</code>
方法将缩放后的预测数据转换回原始比例。这是必要的，因为模型是在缩放后的数据上训练的。</li>
<li>您从转换后的数据中提取了“Close”列的预测值。</li>
</ul></li>
<li><p><strong>可能的修改</strong>：</p>
<ul>
<li>确保 <code>predict_lstm</code>
函数能够处理单个时间步的输入，并返回正确的预测值。如果 LSTM
单元设计为处理多个时间步，则可能需要修改该函数以适应这一点。</li>
<li>如果您的目标是进行多步预测（即预测未来多个时间点的收盘价），则可能需要修改预测循环以生成多个时间步的预测。</li>
<li>验证 <code>scaler</code>
是否确实包含了用于逆变换所需的所有信息（例如，均值和标准差）。</li>
</ul></li>
<li><p><strong>输出结果</strong>：</p>
<ul>
<li>您打印了预测结果及其平均值。这可以帮助您了解模型的预测性能。</li>
</ul></li>
<li><p><strong>注意</strong>：</p>
<ul>
<li>LSTM
通常用于处理序列数据，并且一次可以处理多个时间步。在您的代码中，您似乎对每个时间步单独进行了预测，这在实际应用中可能不是最优的。</li>
<li>确保 <code>scaler</code>
和其他预处理步骤（如特征选择）在训练和测试数据上保持一致。</li>
</ul></li>
</ol>
<p>以下是一个简化的预测步骤示例，假设 <code>predict_lstm</code>
函数已经能够处理整个序列（而不仅仅是单个时间步）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设 predict_lstm 现在可以处理整个序列 X_test</span><br>predicted_full = predict_lstm(X_test, lstm_cell, W_out, b_out)<br><span class="hljs-comment"># 注意：这里 predicted_full 的形状应该与 scaled_data 的相应部分相匹配</span><br><span class="hljs-comment"># 逆变换和结果提取保持不变</span><br>original_predicted_full = scaler.inverse_transform(predicted_full)<br>original_predicted = original_predicted_full[:, <span class="hljs-number">3</span>]  <span class="hljs-comment"># 提取 &#x27;Close&#x27; 列</span><br>pingjun = np.mean(original_predicted)<br><br><span class="hljs-comment"># 输出结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;预测结果: <span class="hljs-subst">&#123;original_predicted&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;预测结果(平均值): <span class="hljs-subst">&#123;pingjun&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>但是，请注意，这个简化的示例假设 <code>predict_lstm</code>
已经被相应地修改以处理整个序列。如果它仍然只能处理单个时间步，则您可能需要使用循环或其他方法来生成整个序列的预测。</p>
<h2
id="用于抓取股价的python代码">用于抓取股价的<code>python</code>代码</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-    --------------Ashare 股票行情数据双核心版( https://github.com/mpquant/Ashare )</span><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> datetime<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd  <span class="hljs-comment">#</span><br><br><span class="hljs-comment"># 腾讯日线</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_price_day_tx</span>(<span class="hljs-params">code, end_date=<span class="hljs-string">&#x27;&#x27;</span>, count=<span class="hljs-number">10</span>, frequency=<span class="hljs-string">&#x27;1d&#x27;</span></span>):  <span class="hljs-comment"># 日线获取</span><br>    unit = <span class="hljs-string">&#x27;week&#x27;</span> <span class="hljs-keyword">if</span> frequency <span class="hljs-keyword">in</span> <span class="hljs-string">&#x27;1w&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;month&#x27;</span> <span class="hljs-keyword">if</span> frequency <span class="hljs-keyword">in</span> <span class="hljs-string">&#x27;1M&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;day&#x27;</span>  <span class="hljs-comment"># 判断日线，周线，月线</span><br>    <span class="hljs-keyword">if</span> end_date:<br>        end_date = end_date.strftime(<br>            <span class="hljs-string">&#x27;%Y-%m-%d&#x27;</span>) <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(end_date, datetime.date) <span class="hljs-keyword">else</span> end_date.split(<span class="hljs-string">&#x27; &#x27;</span>)[<span class="hljs-number">0</span>]<br>    end_date = <span class="hljs-string">&#x27;&#x27;</span> <span class="hljs-keyword">if</span> end_date == datetime.datetime.now().strftime(<br>        <span class="hljs-string">&#x27;%Y-%m-%d&#x27;</span>) <span class="hljs-keyword">else</span> end_date  <span class="hljs-comment"># 如果日期今天就变成空</span><br>    URL = <span class="hljs-string">f&#x27;http://web.ifzq.gtimg.cn/appstock/app/fqkline/get?param=<span class="hljs-subst">&#123;</span></span><br><span class="hljs-subst"><span class="hljs-string">        code&#125;</span>,<span class="hljs-subst">&#123;unit&#125;</span>,,<span class="hljs-subst">&#123;end_date&#125;</span>,<span class="hljs-subst">&#123;count&#125;</span>,qfq&#x27;</span><br>    st = json.loads(requests.get(URL).content)<br>    ms = <span class="hljs-string">&#x27;qfq&#x27;</span>+unit<br>    stk = st[<span class="hljs-string">&#x27;data&#x27;</span>][code]<br>    buf = stk[ms] <span class="hljs-keyword">if</span> ms <span class="hljs-keyword">in</span> stk <span class="hljs-keyword">else</span> stk[unit]  <span class="hljs-comment"># 指数返回不是qfqday,是day</span><br>    df = pd.DataFrame(<br>        buf, columns=[<span class="hljs-string">&#x27;time&#x27;</span>, <span class="hljs-string">&#x27;open&#x27;</span>, <span class="hljs-string">&#x27;close&#x27;</span>, <span class="hljs-string">&#x27;high&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;volume&#x27;</span>], dtype=<span class="hljs-string">&#x27;float&#x27;</span>)<br>    df.time = pd.to_datetime(df.time)<br>    df.set_index([<span class="hljs-string">&#x27;time&#x27;</span>], inplace=<span class="hljs-literal">True</span>)<br>    df.index.name = <span class="hljs-string">&#x27;&#x27;</span>  <span class="hljs-comment"># 处理索引</span><br>    <span class="hljs-keyword">return</span> df<br><br><span class="hljs-comment"># 腾讯分钟线</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_price_min_tx</span>(<span class="hljs-params">code, end_date=<span class="hljs-literal">None</span>, count=<span class="hljs-number">10</span>, frequency=<span class="hljs-string">&#x27;1d&#x27;</span></span>):  <span class="hljs-comment"># 分钟线获取</span><br>    ts = <span class="hljs-built_in">int</span>(frequency[:-<span class="hljs-number">1</span>]) <span class="hljs-keyword">if</span> frequency[:-<span class="hljs-number">1</span>].isdigit() <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>  <span class="hljs-comment"># 解析K线周期数</span><br>    <span class="hljs-keyword">if</span> end_date:<br>        end_date = end_date.strftime(<br>            <span class="hljs-string">&#x27;%Y-%m-%d&#x27;</span>) <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(end_date, datetime.date) <span class="hljs-keyword">else</span> end_date.split(<span class="hljs-string">&#x27; &#x27;</span>)[<span class="hljs-number">0</span>]<br>    URL = <span class="hljs-string">f&#x27;http://ifzq.gtimg.cn/appstock/app/kline/mkline?param=<span class="hljs-subst">&#123;</span></span><br><span class="hljs-subst"><span class="hljs-string">        code&#125;</span>,m<span class="hljs-subst">&#123;ts&#125;</span>,,<span class="hljs-subst">&#123;count&#125;</span>&#x27;</span><br>    st = json.loads(requests.get(URL).content)<br>    buf = st[<span class="hljs-string">&#x27;data&#x27;</span>][code][<span class="hljs-string">&#x27;m&#x27;</span>+<span class="hljs-built_in">str</span>(ts)]<br>    df = pd.DataFrame(<br>        buf, columns=[<span class="hljs-string">&#x27;time&#x27;</span>, <span class="hljs-string">&#x27;open&#x27;</span>, <span class="hljs-string">&#x27;close&#x27;</span>, <span class="hljs-string">&#x27;high&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;volume&#x27;</span>, <span class="hljs-string">&#x27;n1&#x27;</span>, <span class="hljs-string">&#x27;n2&#x27;</span>])<br>    df = df[[<span class="hljs-string">&#x27;time&#x27;</span>, <span class="hljs-string">&#x27;open&#x27;</span>, <span class="hljs-string">&#x27;close&#x27;</span>, <span class="hljs-string">&#x27;high&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;volume&#x27;</span>]]<br>    df[[<span class="hljs-string">&#x27;open&#x27;</span>, <span class="hljs-string">&#x27;close&#x27;</span>, <span class="hljs-string">&#x27;high&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;volume&#x27;</span>]] = df[[<br>        <span class="hljs-string">&#x27;open&#x27;</span>, <span class="hljs-string">&#x27;close&#x27;</span>, <span class="hljs-string">&#x27;high&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;volume&#x27;</span>]].astype(<span class="hljs-string">&#x27;float&#x27;</span>)<br>    df.time = pd.to_datetime(df.time)<br>    df.set_index([<span class="hljs-string">&#x27;time&#x27;</span>], inplace=<span class="hljs-literal">True</span>)<br>    df.index.name = <span class="hljs-string">&#x27;&#x27;</span>  <span class="hljs-comment"># 处理索引</span><br>    df[<span class="hljs-string">&#x27;close&#x27;</span>][-<span class="hljs-number">1</span>] = <span class="hljs-built_in">float</span>(st[<span class="hljs-string">&#x27;data&#x27;</span>][code][<span class="hljs-string">&#x27;qt&#x27;</span>][code][<span class="hljs-number">3</span>])  <span class="hljs-comment"># 最新基金数据是3位的</span><br>    <span class="hljs-keyword">return</span> df<br><br><br><span class="hljs-comment"># sina新浪全周期获取函数，分钟线 5m,15m,30m,60m  日线1d=240m   周线1w=1200m  1月=7200m</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_price_sina</span>(<span class="hljs-params">code, end_date=<span class="hljs-string">&#x27;&#x27;</span>, count=<span class="hljs-number">10</span>, frequency=<span class="hljs-string">&#x27;60m&#x27;</span></span>):  <span class="hljs-comment"># 新浪全周期获取函数</span><br>    frequency = frequency.replace(<span class="hljs-string">&#x27;1d&#x27;</span>, <span class="hljs-string">&#x27;240m&#x27;</span>).replace(<br>        <span class="hljs-string">&#x27;1w&#x27;</span>, <span class="hljs-string">&#x27;1200m&#x27;</span>).replace(<span class="hljs-string">&#x27;1M&#x27;</span>, <span class="hljs-string">&#x27;7200m&#x27;</span>)<br>    mcount = count<br>    ts = <span class="hljs-built_in">int</span>(frequency[:-<span class="hljs-number">1</span>]) <span class="hljs-keyword">if</span> frequency[:-<span class="hljs-number">1</span>].isdigit() <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>  <span class="hljs-comment"># 解析K线周期数</span><br>    <span class="hljs-keyword">if</span> (end_date != <span class="hljs-string">&#x27;&#x27;</span>) &amp; (frequency <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;240m&#x27;</span>, <span class="hljs-string">&#x27;1200m&#x27;</span>, <span class="hljs-string">&#x27;7200m&#x27;</span>]):<br>        end_date = pd.to_datetime(end_date) <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(<br>            end_date, datetime.date) <span class="hljs-keyword">else</span> end_date  <span class="hljs-comment"># 转换成datetime</span><br>        unit = <span class="hljs-number">4</span> <span class="hljs-keyword">if</span> frequency == <span class="hljs-string">&#x27;1200m&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-number">29</span> <span class="hljs-keyword">if</span> frequency == <span class="hljs-string">&#x27;7200m&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span><br>        <span class="hljs-comment"># 4,29多几个数据不影响速度</span><br>        <span class="hljs-comment"># 结束时间到今天有多少天自然日(肯定 &gt;交易日)</span><br>        count = count+(datetime.datetime.now()-end_date).days//unit<br>        <span class="hljs-comment"># print(code,end_date,count)</span><br>    URL = <span class="hljs-string">f&#x27;http://money.finance.sina.com.cn/quotes_service/api/json_v2.php/CN_MarketData.getKLineData?symbol=<span class="hljs-subst">&#123;</span></span><br><span class="hljs-subst"><span class="hljs-string">        code&#125;</span>&amp;scale=<span class="hljs-subst">&#123;ts&#125;</span>&amp;ma=5&amp;datalen=<span class="hljs-subst">&#123;count&#125;</span>&#x27;</span><br>    dstr = json.loads(requests.get(URL).content)<br>    <span class="hljs-comment"># df=pd.DataFrame(dstr,columns=[&#x27;day&#x27;,&#x27;open&#x27;,&#x27;high&#x27;,&#x27;low&#x27;,&#x27;close&#x27;,&#x27;volume&#x27;],dtype=&#x27;float&#x27;)</span><br>    df = pd.DataFrame(<br>        dstr, columns=[<span class="hljs-string">&#x27;day&#x27;</span>, <span class="hljs-string">&#x27;open&#x27;</span>, <span class="hljs-string">&#x27;high&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;close&#x27;</span>, <span class="hljs-string">&#x27;volume&#x27;</span>])<br>    df[<span class="hljs-string">&#x27;open&#x27;</span>] = df[<span class="hljs-string">&#x27;open&#x27;</span>].astype(<span class="hljs-built_in">float</span>)<br>    df[<span class="hljs-string">&#x27;high&#x27;</span>] = df[<span class="hljs-string">&#x27;high&#x27;</span>].astype(<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># 转换数据类型</span><br>    df[<span class="hljs-string">&#x27;low&#x27;</span>] = df[<span class="hljs-string">&#x27;low&#x27;</span>].astype(<span class="hljs-built_in">float</span>)<br>    df[<span class="hljs-string">&#x27;close&#x27;</span>] = df[<span class="hljs-string">&#x27;close&#x27;</span>].astype(<span class="hljs-built_in">float</span>)<br>    df[<span class="hljs-string">&#x27;volume&#x27;</span>] = df[<span class="hljs-string">&#x27;volume&#x27;</span>].astype(<span class="hljs-built_in">float</span>)<br>    df.day = pd.to_datetime(df.day)<br>    df.set_index([<span class="hljs-string">&#x27;day&#x27;</span>], inplace=<span class="hljs-literal">True</span>)<br>    df.index.name = <span class="hljs-string">&#x27;&#x27;</span>  <span class="hljs-comment"># 处理索引</span><br>    <span class="hljs-keyword">if</span> (end_date != <span class="hljs-string">&#x27;&#x27;</span>) &amp; (frequency <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;240m&#x27;</span>, <span class="hljs-string">&#x27;1200m&#x27;</span>, <span class="hljs-string">&#x27;7200m&#x27;</span>]):<br>        <span class="hljs-keyword">return</span> df[df.index &lt;= end_date][-mcount:]  <span class="hljs-comment"># 日线带结束时间先返回</span><br>    <span class="hljs-keyword">return</span> df<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_price</span>(<span class="hljs-params">code, end_date=<span class="hljs-string">&#x27;&#x27;</span>, count=<span class="hljs-number">10</span>, frequency=<span class="hljs-string">&#x27;1d&#x27;</span>, fields=[]</span>):<br>    <span class="hljs-comment"># 对外暴露只有唯一函数，这样对用户才是最友好的</span><br>    xcode = code.replace(<span class="hljs-string">&#x27;.XSHG&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>).replace(<span class="hljs-string">&#x27;.XSHE&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)  <span class="hljs-comment"># 证券代码编码兼容处理</span><br>    xcode = <span class="hljs-string">&#x27;sh&#x27;</span>+xcode <span class="hljs-keyword">if</span> (<span class="hljs-string">&#x27;XSHG&#x27;</span> <span class="hljs-keyword">in</span> code) <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;sz&#x27;</span> + \<br>        xcode <span class="hljs-keyword">if</span> (<span class="hljs-string">&#x27;XSHE&#x27;</span> <span class="hljs-keyword">in</span> code) <span class="hljs-keyword">else</span> code<br><br>    <span class="hljs-keyword">if</span> frequency <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;1d&#x27;</span>, <span class="hljs-string">&#x27;1w&#x27;</span>, <span class="hljs-string">&#x27;1M&#x27;</span>]:  <span class="hljs-comment"># 1d日线  1w周线  1M月线</span><br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-comment"># 主力</span><br>            <span class="hljs-keyword">return</span> get_price_sina(xcode, end_date=end_date, count=count, frequency=frequency)<br>        <span class="hljs-keyword">except</span>:<br>            <span class="hljs-comment"># 备用</span><br>            <span class="hljs-keyword">return</span> get_price_day_tx(xcode, end_date=end_date, count=count, frequency=frequency)<br><br>    <span class="hljs-keyword">if</span> frequency <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;1m&#x27;</span>, <span class="hljs-string">&#x27;5m&#x27;</span>, <span class="hljs-string">&#x27;15m&#x27;</span>, <span class="hljs-string">&#x27;30m&#x27;</span>, <span class="hljs-string">&#x27;60m&#x27;</span>]:  <span class="hljs-comment"># 分钟线 ,1m只有腾讯接口  5分钟5m   60分钟60m</span><br>        <span class="hljs-keyword">if</span> frequency <span class="hljs-keyword">in</span> <span class="hljs-string">&#x27;1m&#x27;</span>:<br>            <span class="hljs-keyword">return</span> get_price_min_tx(xcode, end_date=end_date, count=count, frequency=frequency)<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-comment"># 主力</span><br>            <span class="hljs-keyword">return</span> get_price_sina(xcode, end_date=end_date, count=count, frequency=frequency)<br>        <span class="hljs-keyword">except</span>:<br>            <span class="hljs-comment"># 备用</span><br>            <span class="hljs-keyword">return</span> get_price_min_tx(xcode, end_date=end_date, count=count, frequency=frequency)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    df = get_price(<span class="hljs-string">&#x27;sh000001&#x27;</span>, frequency=<span class="hljs-string">&#x27;1d&#x27;</span>,<br>                   count=<span class="hljs-number">10</span>)  <span class="hljs-comment"># 支持&#x27;1d&#x27;日, &#x27;1w&#x27;周, &#x27;1M&#x27;月</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;上证指数日线行情\n&#x27;</span>, df)<br><br>    <span class="hljs-comment"># 支持&#x27;1m&#x27;,&#x27;5m&#x27;,&#x27;15m&#x27;,&#x27;30m&#x27;,&#x27;60m&#x27;</span><br>    df = get_price(<span class="hljs-string">&#x27;000001.XSHG&#x27;</span>, frequency=<span class="hljs-string">&#x27;15m&#x27;</span>, count=<span class="hljs-number">10</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;上证指数分钟线\n&#x27;</span>, df)<br><br><span class="hljs-comment"># Ashare 股票行情数据( https://github.com/mpquant/Ashare )</span><br><br><br><br><span class="hljs-comment"># 具体用法</span><br><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><span class="hljs-string">&quot;&quot;&quot;获取股票历史价格&quot;&quot;&quot;</span><br><span class="hljs-keyword">import</span> Ashare<br><br>gupiao = <span class="hljs-built_in">input</span>(<span class="hljs-string">&#x27;输入代码：&#x27;</span>)<br><span class="hljs-comment"># count为想要获取的天数</span><br><span class="hljs-keyword">if</span> gupiao[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;6&#x27;</span>:<br>	df = Ashare.get_price(<span class="hljs-string">f&#x27;sh<span class="hljs-subst">&#123;gupiao&#125;</span>&#x27;</span>, frequency=<span class="hljs-string">&#x27;1d&#x27;</span>, count=<span class="hljs-number">100</span>)<br><span class="hljs-keyword">else</span>:<br>    df = Ashare.get_price(<span class="hljs-string">f&#x27;sz<span class="hljs-subst">&#123;gupiao&#125;</span>&#x27;</span>, frequency=<span class="hljs-string">&#x27;1d&#x27;</span>, count=<span class="hljs-number">100</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;上证指数历史行情\n&#x27;</span>, df)<br><br>output_File = <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;gupiao&#125;</span>&#x27;</span><br>df.to_csv(<span class="hljs-string">f&#x27;data_<span class="hljs-subst">&#123;output_File&#125;</span>.csv&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;数据已经存储到data_<span class="hljs-subst">&#123;output_File&#125;</span>.csv&quot;</span>)<br></code></pre></td></tr></table></figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" class="category-chain-item">计算机</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%BC%96%E7%A8%8B/" class="print-no-link">#编程</a>
      
        <a href="/tags/python/" class="print-no-link">#python</a>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>神经元学习</div>
      <div>http://example.com/2024/10/04/2024-10-04-神经元学习/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>zqten</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年10月4日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/10/05/2024-10-05-%E7%9B%B8%E5%AF%B9%E8%AE%BA%E7%9A%84%E7%AE%80%E5%8D%95%E8%A7%A3%E9%87%8A/" title="相对论的简单解释">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">相对论的简单解释</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/10/04/2024-10-04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" title="人工智能学习笔记">
                        <span class="hidden-mobile">人工智能学习笔记</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://zqten.github.io/" target="_blank" rel="nofollow noopener"><span>Zqten</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Freedom-Fluid</span></a>  
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
